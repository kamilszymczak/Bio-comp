
@online{adegeoWindowsFormsNET,
  title = {Windows {{Forms}} for .{{NET}} 5 Documentation},
  author = {{adegeo}},
  url = {https://docs.microsoft.com/en-us/dotnet/desktop/winforms/},
  urldate = {2020-10-29},
  abstract = {Learn about using Windows Forms, an open-source, graphical user interface for Windows, on .NET 5.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\X3C3YS7C\\winforms.html},
  langid = {american}
}

@article{albericioCnvlutinIneffectualneuronfreeDeep2016,
  title = {Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing},
  shorttitle = {Cnvlutin},
  author = {Albericio, Jorge and Judd, Patrick and Hetherington, Tayler and Aamodt, Tor and Jerger, Natalie Enright and Moshovos, Andreas},
  date = {2016-06-18},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  shortjournal = {SIGARCH Comput. Archit. News},
  volume = {44},
  pages = {1--13},
  issn = {0163-5964},
  doi = {10.1145/3007787.3001138},
  url = {https://doi.org/10.1145/3007787.3001138},
  urldate = {2020-11-02},
  abstract = {This work observes that a large fraction of the computations performed by Deep Neural Networks (DNNs) are intrinsically ineffectual as they involve a multiplication where one of the inputs is zero. This observation motivates Cnvlutin (CNV), a value-based approach to hardware acceleration that eliminates most of these ineffectual operations, improving performance and energy over a state-of-the-art accelerator with no accuracy loss. CNV uses hierarchical data-parallel units, allowing groups of lanes to proceed mostly independently enabling them to skip over the ineffectual computations. A co-designed data storage format encodes the computation elimination decisions taking them off the critical path while avoiding control divergence in the data parallel units. Combined, the units and the data storage format result in a data-parallel architecture that maintains wide, aligned accesses to its memory hierarchy and that keeps its data lanes busy. By loosening the ineffectual computation identification criterion, CNV enables further performance and energy efficiency improvements, and more so if a loss in accuracy is acceptable. Experimental measurements over a set of state-of-the-art DNNs for image classification show that CNV improves performance over a state-of-the-art accelerator from 1.24× to 1.55× and by 1.37× on average without any loss in accuracy by removing zero-valued operand multiplications alone. While CNV incurs an area overhead of 4.49\%, it improves overall EDP (Energy Delay Product) and ED2P (Energy Delay Squared Product) on average by 1.47× and 2.01×, respectively. The average performance improvements increase to 1.52× without any loss in accuracy with a broader ineffectual identification policy. Further improvements are demonstrated with a loss in accuracy.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\JL376XPD\\Albericio et al. - 2016 - Cnvlutin ineffectual-neuron-free deep neural netw.pdf},
  keywords = {Inference acccelerator},
  number = {3}
}

@online{AngleSharpHome,
  title = {{{AngleSharp}} - {{Home}}},
  url = {https://anglesharp.github.io/},
  urldate = {2020-10-29},
  file = {C\:\\Users\\fried\\Zotero\\storage\\KLJVLA4D\\anglesharp.github.io.html}
}

@article{bacchusPerformanceMetricsApproximate,
  title = {Performance {{Metrics}} for {{Approximate Deep Learning}} on {{Programmable Hardware}}},
  author = {Bacchus, Denis Pascal},
  pages = {47},
  abstract = {Neural networks are increasingly used in image recognition, autonomous systems, language processing. The computational requirement for training and deploying neural networks has increased a lot over the last decade because for neural networks these application domains have grown from single hidden layer topologies with several hundred weights to deep models reaching one hundred hidden layers with millions of weights. This prohibits the use of embedded system accelerators for executing “out of the box” full precision neural networks because they have limited hardware resources for storage and computation. This is unfortunate because embedded and programmable hardware, e.g Field Programmable Gate Arrays (FPGA) are capable of performing computations extremely quickly, at a low energy cost. For neural networks, this means a high-speed classification speed, e.g classifying thousands of images each second.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\24WVBTGN\\Bacchus - Performance Metrics for Approximate Deep Learning .pdf},
  langid = {english}
}

@inproceedings{baiHighSpeedEnergy2019,
  title = {High Speed and Energy Efficient Deep Neural Network for Edge Computing},
  booktitle = {Proceedings of the 4th {{ACM}}/{{IEEE Symposium}} on {{Edge Computing}}},
  author = {Bai, Kangjun and Liu, Shiya and Yi, Yang},
  date = {2019-11-07},
  pages = {347--349},
  publisher = {{ACM}},
  location = {{Arlington Virginia}},
  doi = {10.1145/3318216.3363453},
  url = {https://dl.acm.org/doi/10.1145/3318216.3363453},
  urldate = {2020-10-01},
  abstract = {Edge computing enables data-stream acceleration with realtime data processing without latency, and allows for efficient data processing in that large amounts of data can be processed near the source with the ability to process data without ever putting it into a public cloud adds a useful layer of security for sensitive data. The edge computing-based architecture design and analysis play key impacts for the future Internet of Things (IoT) infrastructure development. In this work, we design a low power hybrid structured deep neural network (Hybrid-DNN), which employs memristive synapses working in a hierarchical information processing fashion and delay-based spiking neural network (SNN) modules as the readout layer, and provide a novel data layout method to allow the Hybrid DNN running a computationally intensive deep learning algorithm on limited resource edge devices. Motivated by the recent findings in neuromorphic computing and edge computing, we design a hybrid structured DNN combining both depth-in-space (spatial) and depth-in-time (temporal) deep learning architectures. Our Hybrid-DNN employs memristive synapses working in a hierarchical information processing fashion and delay-based spiking neural network modules as the readout layer.},
  eventtitle = {{{SEC}} '19: {{The Fourth ACM}}/{{IEEE Symposium}} on {{Edge Computing}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\H9DIBH63\\Bai et al. - 2019 - High speed and energy efficient deep neural networ.pdf},
  isbn = {978-1-4503-6733-2},
  langid = {english}
}

@online{billwagnerDelegatesProgrammingGuide,
  title = {Delegates - {{C}}\# {{Programming Guide}}},
  author = {BillWagner},
  url = {https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/delegates/},
  urldate = {2020-10-29},
  abstract = {A delegate in C\# is a type that refers to methods with a parameter list and return type. Delegates are used to pass methods as arguments to other methods.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\SQWUEBII\\delegates.html},
  langid = {american}
}

@online{billwagnerSealedModifierReference,
  title = {Sealed Modifier - {{C}}\# {{Reference}}},
  author = {BillWagner},
  url = {https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/sealed},
  urldate = {2020-10-29},
  abstract = {sealed modifier - C\# Reference},
  file = {C\:\\Users\\fried\\Zotero\\storage\\V9NRHGNQ\\sealed.html},
  langid = {american}
}

@article{blottFINNEndtoEndDeepLearning2018,
  title = {{{FINN}}- {{{\emph{R}}}}: {{An End}}-to-{{End Deep}}-{{Learning Framework}} for {{Fast Exploration}} of {{Quantized Neural Networks}}},
  shorttitle = {{{FINN}}- {{{\emph{R}}}}},
  author = {Blott, Michaela and Preußer, Thomas B. and Fraser, Nicholas J. and Gambardella, Giulio and O’brien, Kenneth and Umuroglu, Yaman and Leeser, Miriam and Vissers, Kees},
  date = {2018-12-22},
  journaltitle = {ACM Transactions on Reconfigurable Technology and Systems},
  shortjournal = {ACM Trans. Reconfigurable Technol. Syst.},
  volume = {11},
  pages = {1--23},
  issn = {1936-7406, 1936-7414},
  doi = {10.1145/3242897},
  url = {https://dl.acm.org/doi/10.1145/3242897},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\HI8E3LG4\\Blott et al. - 2018 - FINN- R An End-to-End Deep-Learning Framew.pdf},
  langid = {english},
  number = {3}
}

@inproceedings{bottouComparisonClassifierMethods1994,
  title = {Comparison of Classifier Methods: A Case Study in Handwritten Digit Recognition},
  shorttitle = {Comparison of Classifier Methods},
  booktitle = {Proceedings of the 12th {{IAPR International Conference}} on {{Pattern Recognition}} ({{Cat}}. {{No}}.{{94CH3440}}-5)},
  author = {Bottou, L. and Cortes, C. and Denker, J.S. and Drucker, H. and Guyon, I. and Jackel, L.D. and LeCun, Y. and Muller, U.A. and Sackinger, E. and Simard, P. and Vapnik, V.},
  date = {1994},
  volume = {2},
  pages = {77--82},
  publisher = {{IEEE Comput. Soc. Press}},
  location = {{Jerusalem, Israel}},
  doi = {10.1109/ICPR.1994.576879},
  url = {http://ieeexplore.ieee.org/document/576879/},
  urldate = {2020-10-18},
  abstract = {This paper compares the performance of several classiJier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have miscIassiJication rates less than a given threshold.},
  eventtitle = {12th {{International Conference}} on {{Pattern Recognition}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\PT3JQB2P\\Bottou et al. - 1994 - Comparison of classifier methods a case study in .pdf},
  isbn = {978-0-8186-6270-6},
  langid = {english}
}

@article{chenDeepLearningEdge2019,
  title = {Deep {{Learning With Edge Computing}}: {{A Review}}},
  shorttitle = {Deep {{Learning With Edge Computing}}},
  author = {Chen, Jiasi and Ran, Xukan},
  date = {2019-08},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {107},
  pages = {1655--1674},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2019.2921977},
  url = {https://ieeexplore.ieee.org/document/8763885/},
  urldate = {2020-10-01},
  abstract = {Deep learning is currently widely used in a variety of applications, including computer vision and natural language processing. End devices, such as smartphones and Internet-of-Things sensors, are generating data that need to be analyzed in real time using deep learning or used to train deep learning models. However, deep learning inference and training require substantial computation resources to run quickly. Edge computing, where a fine mesh of compute nodes are placed close to end devices, is a viable way to meet the high computation and low-latency requirements of deep learning on edge devices and also provides additional benefits in terms of privacy, bandwidth efficiency, and scalability. This paper aims to provide a comprehensive review of the current state of the art at the intersection of deep learning and edge computing. Specifically, it will provide an overview of applications where deep learning is used at the network edge, discuss various approaches for quickly executing deep learning inference across a combination of end devices, edge servers, and the cloud, and describe the methods for training deep learning models across multiple edge devices. It will also discuss open challenges in terms of systems performance, network technologies and management, benchmarks, and privacy. The reader will take away the following concepts from this paper: understanding scenarios where deep learning at the network edge can be useful, understanding common techniques for speeding up deep learning inference and performing distributed training on edge devices, and understanding recent trends and opportunities.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\KY7Z8NX2\\Chen and Ran - 2019 - Deep Learning With Edge Computing A Review.pdf},
  langid = {english},
  number = {8}
}

@article{chenDeepLearningMobile2020,
  ids = {chenDeepLearningMobile2020a},
  title = {Deep {{Learning}} on {{Mobile}} and {{Embedded Devices}}: {{State}}-of-the-Art, {{Challenges}}, and {{Future Directions}}},
  shorttitle = {Deep {{Learning}} on {{Mobile}} and {{Embedded Devices}}},
  author = {Chen, Yanjiao and Zheng, Baolin and Zhang, Zihan and Wang, Qian and Shen, Chao and Zhang, Qian},
  date = {2020-09-26},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3398209},
  url = {https://dl.acm.org/doi/10.1145/3398209},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\BL8MAH7E\\Chen et al. - 2020 - Deep Learning on Mobile and Embedded Devices Stat.pdf;C\:\\Users\\fried\\Zotero\\storage\\UV8LBZCW\\Chen et al. - 2020 - Deep Learning on Mobile and Embedded Devices Stat.pdf},
  langid = {english},
  number = {4}
}

@article{chenEyerissEnergyEfficientReconfigurable2017,
  title = {Eyeriss: {{An Energy}}-{{Efficient Reconfigurable Accelerator}} for {{Deep Convolutional Neural Networks}}},
  shorttitle = {Eyeriss},
  author = {Chen, Y. and Krishna, T. and Emer, J. S. and Sze, V.},
  date = {2017-01},
  journaltitle = {IEEE Journal of Solid-State Circuits},
  volume = {52},
  pages = {127--138},
  issn = {1558-173X},
  doi = {10.1109/JSSC.2016.2616357},
  abstract = {Eyeriss is an accelerator for state-of-the-art deep convolutional neural networks (CNNs). It optimizes for the energy efficiency of the entire system, including the accelerator chip and off-chip DRAM, for various CNN shapes by reconfiguring the architecture. CNNs are widely used in modern AI systems but also bring challenges on throughput and energy efficiency to the underlying hardware. This is because its computation requires a large amount of data, creating significant data movement from on-chip and off-chip that is more energy-consuming than computation. Minimizing data movement energy cost for any CNN shape, therefore, is the key to high throughput and energy efficiency. Eyeriss achieves these goals by using a proposed processing dataflow, called row stationary (RS), on a spatial architecture with 168 processing elements. RS dataflow reconfigures the computation mapping of a given shape, which optimizes energy efficiency by maximally reusing data locally to reduce expensive data movement, such as DRAM accesses. Compression and data gating are also applied to further improve energy efficiency. Eyeriss processes the convolutional layers at 35 frames/s and 0.0029 DRAM access/multiply and accumulation (MAC) for AlexNet at 278 mW (batch size N = 4), and 0.7 frames/s and 0.0035 DRAM access/MAC for VGG-16 at 236 mW (N = 3).},
  eventtitle = {{{IEEE Journal}} of {{Solid}}-{{State Circuits}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\CZ78IMWL\\Chen et al. - 2017 - Eyeriss An Energy-Efficient Reconfigurable Accele.pdf;C\:\\Users\\fried\\Zotero\\storage\\8FZIA8JF\\7738524.html},
  keywords = {accelerator chip,AI systems,AlexNet,Clocks,CNN shapes,Computer architecture,convolutional layers,Convolutional neural networks (CNNs),data flow computing,data movement energy cost,dataflow processing,deep convolutional neural networks,deep learning,DRAM accesses,DRAM chips,energy conservation,energy efficiency,energy-efficient accelerators,energy-efficient reconfigurable accelerator,Eyeriss,feedforward neural nets,Hardware,learning (artificial intelligence),MAC,multiply and accumulation,neural net architecture,Neural networks,off-chip DRAM,power aware computing,Random access memory,reconfigurable architectures,reconfiguring architecture,row stationary,RS dataflow reconfiguration,Shape,spatial architecture,Throughput},
  number = {1}
}

@article{chenEyerissSpatialArchitecture,
  title = {Eyeriss: {{A Spatial Architecture}} for {{Energy}}-{{Efﬁcient Dataﬂow}} for {{Convolutional Neural Networks}}},
  author = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
  pages = {14},
  abstract = {Deep convolutional neural networks (CNNs) are widely used in modern AI systems for their superior accuracy but at the cost of high computational complexity. The complexity comes from the need to simultaneously process hundreds of filters and channels in the high-dimensional convolutions, which involve a significant amount of data movement. Although highly-parallel compute paradigms, such as SIMD/SIMT, effectively address the computation requirement to achieve high throughput, energy consumption still remains high as data movement can be more expensive than computation. Accordingly, finding a dataflow that supports parallel processing with minimal data movement cost is crucial to achieving energyefficient CNN processing without compromising accuracy. In this paper, we present a novel dataflow, called rowstationary (RS), that minimizes data movement energy consumption on a spatial architecture. This is realized by exploiting local data reuse of filter weights and feature map pixels, i.e., activations, in the high-dimensional convolutions, and minimizing data movement of partial sum accumulations. Unlike dataflows used in existing designs, which only reduce certain types of data movement, the proposed RS dataflow can adapt to different CNN shape configurations and reduces all types of data movement through maximally utilizing the processing engine (PE) local storage, direct inter-PE communication and spatial parallelism. To evaluate the energy efficiency of the different dataflows, we propose an analysis framework that compares energy cost under the same hardware area and processing parallelism constraints. Experiments using the CNN configurations of AlexNet show that the proposed RS dataflow is more energy efficient than existing dataflows in both convolutional (1.4× to 2.5×) and fully-connected layers (at least 1.3× for batch size larger than 16). The RS dataflow has also been demonstrated on a fabricated chip, which verifies our energy analysis.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\GU828577\\Chen et al. - Eyeriss A Spatial Architecture for Energy-Efﬁcien.pdf},
  keywords = {Inference acccelerator},
  langid = {english}
}

@article{chenEyerissV2Flexible2019,
  title = {Eyeriss v2: {{A Flexible Accelerator}} for {{Emerging Deep Neural Networks}} on {{Mobile Devices}}},
  shorttitle = {Eyeriss V2},
  author = {Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S. and Sze, Vivienne},
  date = {2019-06},
  journaltitle = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  shortjournal = {IEEE J. Emerg. Sel. Topics Circuits Syst.},
  volume = {9},
  pages = {292--308},
  issn = {2156-3357, 2156-3365},
  doi = {10.1109/JETCAS.2019.2910232},
  url = {https://ieeexplore.ieee.org/document/8686088/},
  urldate = {2020-10-01},
  abstract = {A recent trend in deep neural network (DNN) development is to extend the reach of deep learning applications to platforms that are more resource and energy-constrained, e.g., mobile devices. These endeavors aim to reduce the DNN model size and improve the hardware processing efficiency and have resulted in DNNs that are much more compact in their structures and/or have high data sparsity. These compact or sparse models are different from the traditional large ones in that there is much more variation in their layer shapes and sizes and often require specialized hardware to exploit sparsity for performance improvement. Therefore, many DNN accelerators designed for large DNNs do not perform well on these models. In this paper, we present Eyeriss v2, a DNN accelerator architecture designed for running compact and sparse DNNs. To deal with the widely varying layer shapes and sizes, it introduces a highly flexible on-chip network, called hierarchical mesh, that can adapt to the different amounts of data reuse and bandwidth requirements of different data types, which improves the utilization of the computation resources. Furthermore, Eyeriss v2 can process sparse data directly in the compressed domain for both weights and activations and therefore is able to improve both processing speed and energy efficiency with sparse models. Overall, with sparse MobileNet, Eyeriss v2 in a 65-nm CMOS process achieves a throughput of 1470.6 inferences/s and 2560.3 inferences/J at a batch size of 1, which is 12.6× faster and 2.5× more energyefficient than the original Eyeriss running MobileNet.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\6KND8LRJ\\Chen et al. - 2019 - Eyeriss v2 A Flexible Accelerator for Emerging De.pdf},
  keywords = {Inference acccelerator},
  langid = {english},
  number = {2}
}

@article{chenISSCC2016SESSION,
  title = {{{ISSCC}} 2016 / {{SESSION}} 14 / {{NEXT}}-{{GENERATION PROCESSING}} / 14.5},
  author = {Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
  pages = {4},
  langid = {english}
}

@article{chenISSCC2016SESSIONa,
  title = {{{ISSCC}} 2016 / {{SESSION}} 14 / {{NEXT}}-{{GENERATION PROCESSING}} / 14.5},
  author = {Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
  pages = {4},
  file = {C\:\\Users\\fried\\Zotero\\storage\\4K7FUYEU\\Chen et al. - ISSCC 2016  SESSION 14  NEXT-GENERATION PROCESSI.pdf},
  langid = {english}
}

@article{chenISSCC2016SESSIONb,
  title = {{{ISSCC}} 2016 / {{SESSION}} 14 / {{NEXT}}-{{GENERATION PROCESSING}} / 14.5},
  author = {Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
  pages = {4},
  file = {C\:\\Users\\fried\\Zotero\\storage\\RX2YCF4L\\Chen et al. - ISSCC 2016  SESSION 14  NEXT-GENERATION PROCESSI.pdf},
  langid = {english}
}

@inproceedings{chenPerformanceEvaluationEdge2018,
  title = {Performance {{Evaluation}} of {{Edge Computing}}-{{Based Deep Learning Object Detection}}},
  booktitle = {Proceedings of the 2018 {{VII International Conference}} on {{Network}}, {{Communication}} and {{Computing}} - {{ICNCC}} 2018},
  author = {Chen, Chuan-Wen and Ruan, Shanq-Jang and Lin, Chang-Hong and Hung, Chun-Chi},
  date = {2018},
  pages = {40--43},
  publisher = {{ACM Press}},
  location = {{Taipei City, Taiwan}},
  doi = {10.1145/3301326.3301369},
  url = {http://dl.acm.org/citation.cfm?doid=3301326.3301369},
  urldate = {2020-10-01},
  abstract = {This article presents a method for implementing the deep learning object detection based on a low-cost edge computing IoT device. The limit of the hardware is a challenge for working the pretrained neural network model on a low-cost IoT device. Hence, we utilize the Neural Compute Stick (NCS) to accelerate the neural network model on a low-cost IoT device by its high efficiency floating-point operation. With the NCS, the low-cost IoT device can successfully work the pre-trained neural network model and become an edge computing device. The experimental results show the proposed method can effectively detect the objects based on deep learning on an edge computing IoT device. Furthermore, the objective experiment demonstrates the proposed method can immediately infer the neural network model for images in average 1.7 seconds with only one of the NCS and the neural network model can reach average 9.2 fps for the video sequences with four NCSs acceleration. In addition, the discrepancy of the neural network model between the edge device and the edge server is less than 2\% mean average precision (mAP).},
  eventtitle = {The 2018 {{VII International Conference}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\RPMSVR79\\Chen et al. - 2018 - Performance Evaluation of Edge Computing-Based Dee.pdf},
  isbn = {978-1-4503-6553-6},
  langid = {english}
}

@article{chiPRIMENovelProcessinginmemory2016,
  title = {{{PRIME}}: A Novel Processing-in-Memory Architecture for Neural Network Computation in {{ReRAM}}-Based Main Memory},
  shorttitle = {{{PRIME}}},
  author = {Chi, Ping and Li, Shuangchen and Xu, Cong and Zhang, Tao and Zhao, Jishen and Liu, Yongpan and Wang, Yu and Xie, Yuan},
  date = {2016-06-18},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  shortjournal = {SIGARCH Comput. Archit. News},
  volume = {44},
  pages = {27--39},
  issn = {0163-5964},
  doi = {10.1145/3007787.3001140},
  url = {https://doi.org/10.1145/3007787.3001140},
  urldate = {2020-11-02},
  abstract = {Processing-in-memory (PIM) is a promising solution to address the "memory wall" challenges for future computer systems. Prior proposed PIM architectures put additional computation logic in or near memory. The emerging metal-oxide resistive random access memory (ReRAM) has showed its potential to be used for main memory. Moreover, with its crossbar array structure, ReRAM can perform matrix-vector multiplication efficiently, and has been widely studied to accelerate neural network (NN) applications. In this work, we propose a novel PIM architecture, called PRIME, to accelerate NN applications in ReRAM based main memory. In PRIME, a portion of ReRAM crossbar arrays can be configured as accelerators for NN applications or as normal memory for a larger memory space. We provide microarchitecture and circuit designs to enable the morphable functions with an insignificant area overhead. We also design a software/hardware interface for software developers to implement various NNs on PRIME. Benefiting from both the PIM architecture and the efficiency of using ReRAM for NN computation, PRIME distinguishes itself from prior work on NN acceleration, with significant performance improvement and energy saving. Our experimental results show that, compared with a state-of-the-art neural processing unit design, PRIME improves the performance by \textasciitilde 2360× and the energy consumption by \textasciitilde 895×, across the evaluated machine learning benchmarks.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\TD752FGV\\Chi et al. - 2016 - PRIME a novel processing-in-memory architecture f.pdf},
  keywords = {Inference acccelerator,neural network,processing in memory,resistive random access memory},
  number = {3}
}

@online{DeepLearning,
  title = {Deep {{Learning}}},
  url = {https://www.deeplearningbook.org/},
  urldate = {2020-10-30},
  file = {C\:\\Users\\fried\\Zotero\\storage\\ZT3NKBD9\\www.deeplearningbook.org.html}
}

@article{dengTutorialSurveyArchitectures2014,
  title = {A Tutorial Survey of Architectures, Algorithms, and Applications for Deep Learning},
  author = {Deng, Li},
  date = {2014},
  journaltitle = {APSIPA Transactions on Signal and Information Processing},
  shortjournal = {APSIPA trans. signal inf. process.},
  volume = {3},
  pages = {e2},
  issn = {2048-7703},
  doi = {10.1017/atsip.2013.9},
  url = {https://www.cambridge.org/core/product/identifier/S2048770313000097/type/journal_article},
  urldate = {2020-10-16},
  abstract = {In this invited paper, my overview material on the same topic as presented in the plenary overview session of APSIPA-2011 and the tutorial material presented in the same conference [1] are expanded and updated to include more recent developments in deep learning. The previous and the updated materials cover both theory and applications, and analyze its future directions. The goal of this tutorial survey is to introduce the emerging area of deep learning or hierarchical learning to the APSIPA community. Deep learning refers to a class of machine learning techniques, developed largely since 2006, where many stages of non-linear information processing in hierarchical architectures are exploited for pattern classification and for feature learning. In the more recent literature, it is also connected to representation learning, which involves a hierarchy of features or concepts where higherlevel concepts are defined from lower-level ones and where the same lower-level concepts help to define higher-level ones. In this tutorial survey, a brief history of deep learning research is discussed first. Then, a classificatory scheme is developed to analyze and summarize major work reported in the recent deep learning literature. Using this scheme, I provide a taxonomy-oriented survey on the existing deep architectures and algorithms in the literature, and categorize them into three classes: generative, discriminative, and hybrid. Three representative deep architectures – deep autoencoders, deep stacking networks with their generalization to the temporal domain (recurrent networks), and deep neural networks (pretrained with deep belief networks) –one in each of the three classes, are presented in more detail. Next, selected applications of deep learning are reviewed in broad areas of signal and information processing including audio/speech, image/vision, multimodality, language modeling, natural language processing, and information retrieval. Finally, future directions of deep learning are discussed and analyzed.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\ZD344ABD\\Deng - 2014 - A tutorial survey of architectures, algorithms, an.pdf},
  langid = {english}
}

@article{dinelliFPGABasedHardwareAccelerator2019,
  title = {An {{FPGA}}-{{Based Hardware Accelerator}} for {{CNNs Using On}}-{{Chip Memories Only}}: {{Design}} and {{Benchmarking}} with {{Intel Movidius Neural Compute Stick}}},
  shorttitle = {An {{FPGA}}-{{Based Hardware Accelerator}} for {{CNNs Using On}}-{{Chip Memories Only}}},
  author = {Dinelli, Gianmarco and Meoni, Gabriele and Rapuano, Emilio and Benelli, Gionata and Fanucci, Luca},
  date = {2019-10-22},
  journaltitle = {International Journal of Reconfigurable Computing},
  shortjournal = {International Journal of Reconfigurable Computing},
  volume = {2019},
  pages = {1--13},
  issn = {1687-7195, 1687-7209},
  doi = {10.1155/2019/7218758},
  url = {https://www.hindawi.com/journals/ijrc/2019/7218758/},
  urldate = {2020-10-01},
  abstract = {During the last years, convolutional neural networks have been used for different applications, thanks to their potentiality to carry out tasks by using a reduced number of parameters when compared with other deep learning approaches. However, power consumption and memory footprint constraints, typical of on the edge and portable applications, usually collide with accuracy and latency requirements. For such reasons, commercial hardware accelerators have become popular, thanks to their architecture designed for the inference of general convolutional neural network models. Nevertheless, field-programmable gate arrays represent an interesting perspective since they offer the possibility to implement a hardware architecture tailored to a specific convolutional neural network model, with promising results in terms of latency and power consumption. In this article, we propose a full on-chip field-programmable gate array hardware accelerator for a separable convolutional neural network, which was designed for a keyword spotting application. We started from the model implemented in a previous work for the Intel Movidius Neural Compute Stick. For our goals, we appropriately quantized such a model through a bit-true simulation, and we realized a dedicated architecture exclusively using on-chip memories. A benchmark comparing the results on different field-programmable gate array families by Xilinx and Intel with the implementation on the Neural Compute Stick was realized. The analysis shows that better inference time and energy per inference results can be obtained with comparable accuracy at expenses of a higher design effort and development time through the FPGA solution.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\V2FAHBGB\\Dinelli et al. - 2019 - An FPGA-Based Hardware Accelerator for CNNs Using .pdf},
  langid = {english}
}

@online{dotnet-botDataContractSerializerClassSystem,
  title = {{{DataContractSerializer Class}} ({{System}}.{{Runtime}}.{{Serialization}})},
  author = {dotnet- {bot}},
  url = {https://docs.microsoft.com/en-us/dotnet/api/system.runtime.serialization.datacontractserializer},
  urldate = {2020-10-29},
  abstract = {Serializes and deserializes an instance of a type into an XML stream or document using a supplied data contract. This class cannot be inherited.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\97ERHAFI\\system.runtime.serialization.html},
  langid = {american},
  options = {useprefix=true}
}

@online{dotnet-botICollectionInterfaceSystem,
  title = {{{ICollection}}{$<$}{{T}}{$>$} {{Interface}} ({{System}}.{{Collections}}.{{Generic}})},
  author = {dotnet- {bot}},
  url = {https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.icollection-1},
  urldate = {2020-10-29},
  abstract = {Defines methods to manipulate generic collections.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\RMU6G9CX\\system.collections.generic.html},
  langid = {american},
  options = {useprefix=true}
}

@online{dotnet-botIEnumerableInterfaceSystem,
  title = {{{IEnumerable}}{$<$}{{T}}{$>$} {{Interface}} ({{System}}.{{Collections}}.{{Generic}})},
  author = {dotnet- {bot}},
  url = {https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.ienumerable-1},
  urldate = {2020-10-29},
  abstract = {Exposes the enumerator, which supports a simple iteration over a collection of a specified type.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\EXIEHZUS\\system.collections.generic.html},
  langid = {american},
  options = {useprefix=true}
}

@online{dotnet-botIListInterfaceSystem,
  title = {{{IList}}{$<$}{{T}}{$>$} {{Interface}} ({{System}}.{{Collections}}.{{Generic}})},
  author = {dotnet- {bot}},
  url = {https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.ilist-1},
  urldate = {2020-10-29},
  abstract = {Represents a collection of objects that can be individually accessed by index.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\WUBKLGMC\\system.collections.generic.html},
  langid = {american},
  options = {useprefix=true}
}

@online{dotnet-botLazyClassSystem,
  title = {Lazy{$<$}{{T}}{$>$} {{Class}} ({{System}})},
  author = {dotnet- {bot}},
  url = {https://docs.microsoft.com/en-us/dotnet/api/system.lazy-1},
  urldate = {2020-10-29},
  abstract = {Provides support for lazy initialization.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\78SUKK57\\system.html},
  langid = {american},
  options = {useprefix=true}
}

@online{dotnet-botLazyThreadSafetyModeEnumSystem,
  title = {{{LazyThreadSafetyMode Enum}} ({{System}}.{{Threading}})},
  author = {dotnet- {bot}},
  url = {https://docs.microsoft.com/en-us/dotnet/api/system.threading.lazythreadsafetymode},
  urldate = {2020-10-29},
  abstract = {Specifies how a Lazy{$<$}T{$>$} instance synchronizes access among multiple threads.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\5GHPZBIU\\system.threading.html},
  langid = {american},
  options = {useprefix=true}
}

@online{dotnet-botListClassSystem,
  title = {List{$<$}{{T}}{$>$} {{Class}} ({{System}}.{{Collections}}.{{Generic}})},
  author = {dotnet- {bot}},
  url = {https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.list-1},
  urldate = {2020-10-29},
  abstract = {Represents a strongly typed list of objects that can be accessed by index. Provides methods to search, sort, and manipulate lists.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\32UD5BQN\\system.collections.generic.html},
  langid = {american},
  options = {useprefix=true}
}

@online{dotnet-botTaskClassSystem,
  title = {Task{$<$}{{TResult}}{$>$} {{Class}} ({{System}}.{{Threading}}.{{Tasks}})},
  author = {dotnet- {bot}},
  url = {https://docs.microsoft.com/en-us/dotnet/api/system.threading.tasks.task-1},
  urldate = {2020-10-29},
  abstract = {Represents an asynchronous operation that can return a value.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\JUHQBUIR\\system.threading.tasks.html},
  langid = {american},
  options = {useprefix=true}
}

@online{dotnet-botTaskClassSystema,
  title = {Task {{Class}} ({{System}}.{{Threading}}.{{Tasks}})},
  author = {dotnet- {bot}},
  url = {https://docs.microsoft.com/en-us/dotnet/api/system.threading.tasks.task},
  urldate = {2020-10-29},
  abstract = {Represents an asynchronous operation.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\VL3QXYGE\\system.threading.tasks.html},
  langid = {american},
  options = {useprefix=true}
}

@inproceedings{drozdalTrustAutoMLExploring2020,
  ids = {drozdalTrustAutoMLExploring2020a},
  title = {Trust in {{AutoML}}: Exploring Information Needs for Establishing Trust in Automated Machine Learning Systems},
  shorttitle = {Trust in {{AutoML}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Drozdal, Jaimie and Weisz, Justin and Wang, Dakuo and Dass, Gaurav and Yao, Bingsheng and Zhao, Changruo and Muller, Michael and Ju, Lin and Su, Hui},
  date = {2020-03-17},
  pages = {297--307},
  publisher = {{ACM}},
  location = {{Cagliari Italy}},
  doi = {10.1145/3377325.3377501},
  url = {https://dl.acm.org/doi/10.1145/3377325.3377501},
  urldate = {2020-10-01},
  abstract = {We explore trust in a relatively new area of data science: Automated Machine Learning (AutoML). In AutoML, AI methods are used to generate and optimize machine learning models by automatically engineering features, selecting models, and optimizing hyperparameters. In this paper, we seek to understand what kinds of information influence data scientists’ trust in the models produced by AutoML? We operationalize trust as a willingness to deploy a model produced using automated methods. We report results from three studies – qualitative interviews, a controlled experiment, and a card-sorting task – to understand the information needs of data scientists for establishing trust in AutoML systems. We find that including transparency features in an AutoML tool increased user trust and understandability in the tool; and out of all proposed features, model performance metrics and visualizations are the most important information to data scientists when establishing their trust with an AutoML tool.},
  eventtitle = {{{IUI}} '20: 25th {{International Conference}} on {{Intelligent User Interfaces}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\EG7X6AAP\\Drozdal et al. - 2020 - Trust in AutoML exploring information needs for e.pdf;C\:\\Users\\fried\\Zotero\\storage\\IL2ZEJTF\\Drozdal et al. - 2020 - Trust in AutoML exploring information needs for e.pdf},
  isbn = {978-1-4503-7118-6},
  langid = {english}
}

@article{duffOverviewSparseBasic2002,
  title = {An Overview of the Sparse Basic Linear Algebra Subprograms: {{The}} New Standard from the {{BLAS}} Technical Forum},
  shorttitle = {An Overview of the Sparse Basic Linear Algebra Subprograms},
  author = {Duff, Iain S. and Heroux, Michael A. and Pozo, Roldan},
  date = {2002-06-01},
  journaltitle = {ACM Transactions on Mathematical Software},
  shortjournal = {ACM Trans. Math. Softw.},
  volume = {28},
  pages = {239--267},
  issn = {0098-3500},
  doi = {10.1145/567806.567810},
  url = {https://doi.org/10.1145/567806.567810},
  urldate = {2020-11-15},
  abstract = {We discuss the interface design for the Sparse Basic Linear Algebra Subprograms (BLAS), the kernels in the recent standard from the BLAS Technical Forum that are concerned with unstructured sparse matrices. The motivation for such a standard is to encourage portable programming while allowing for library-specific optimizations. In particular, we show how this interface can shield one from concern over the specific storage scheme for the sparse matrix. This design makes it easy to add further functionality to the sparse BLAS in the future.We illustrate the use of the Sparse BLAS with examples in the three supported programming languages, Fortran 95, Fortran 77, and C.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\QBRQLIN2\\Duff et al. - 2002 - An overview of the sparse basic linear algebra sub.pdf},
  keywords = {Algorithms,computational kernels,software,sparse BLAS,sparse iterative methods,sparse matrices},
  number = {2}
}

@article{el-sayedEdgeThingsBig2018,
  title = {Edge of {{Things}}: {{The Big Picture}} on the {{Integration}} of {{Edge}}, {{IoT}} and the {{Cloud}} in a {{Distributed Computing Environment}}},
  shorttitle = {Edge of {{Things}}},
  author = {El-Sayed, Hesham and Sankar, Sharmi and Prasad, Mukesh and Puthal, Deepak and Gupta, Akshansh and Mohanty, Manoranjan and Lin, Chin-Teng},
  date = {2018},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {6},
  pages = {1706--1717},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2017.2780087},
  url = {https://ieeexplore.ieee.org/document/8166730/},
  urldate = {2020-10-01},
  abstract = {A centralized infrastructure system carries out existing data analytics and decision-making processes from our current highly virtualized platform of wireless networks and the Internet of Things (IoT) applications. There is a high possibility that these existing methods will encounter more challenges and issues in relation to network dynamics, resulting in a high overhead in the network response time, leading to latency and traffic. In order to avoid these problems in the network and achieve an optimum level of resource utilization, a new paradigm called edge computing (EC) is proposed to pave the way for the evolution of new age applications and services. With the integration of EC, the processing capabilities are pushed to the edge of network devices such as smart phones, sensor nodes, wearables, and on-board units, where data analytics and knowledge generation are performed which removes the necessity for a centralized system. Many IoT applications, such as smart cities, the smart grid, smart traffic lights, and smart vehicles, are rapidly upgrading their applications with EC, significantly improving response time as well as conserving network resources. Irrespective of the fact that EC shifts the workload from a centralized cloud to the edge, the analogy between EC and the cloud pertaining to factors such as resource management and computation optimization are still open to research studies. Hence, this paper aims to validate the efficiency and resourcefulness of EC. We extensively survey the edge systems and present a comparative study of cloud computing systems. After analyzing the different network properties in the system, the results show that EC systems perform better than cloud computing systems. Finally, the research challenges in implementing an EC system and future research directions are discussed.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\4TMVG3HH\\El-Sayed et al. - 2018 - Edge of Things The Big Picture on the Integration.pdf},
  langid = {english}
}

@online{EssentialsMetaheuristics,
  title = {Essentials of {{Metaheuristics}}},
  url = {https://cs.gmu.edu/~sean/book/metaheuristics/},
  urldate = {2020-11-22},
  file = {C\:\\Users\\fried\\Zotero\\storage\\SNC8XQUF\\metaheuristics.html}
}

@article{fukushimaNeocognitronHierarchicalNeural1988,
  title = {Neocognitron: {{A}} Hierarchical Neural Network Capable of Visual Pattern Recognition},
  shorttitle = {Neocognitron},
  author = {Fukushima, Kunihiko},
  date = {1988-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {1},
  pages = {119--130},
  issn = {08936080},
  doi = {10.1016/0893-6080(88)90014-7},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0893608088900147},
  urldate = {2020-10-18},
  abstract = {A neural network modelfor visual pattern recognition, called the "neocognitron, "' was previously proposed by the author In this paper, we discuss the mechanism of the model in detail. In order to demonstrate the ability of the neocognitron, we also discuss a pattern-recognition system which works with the mechanism o f the neocognitron.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\N4AJE4XB\\Fukushima - 1988 - Neocognitron A hierarchical neural network capabl.pdf},
  langid = {english},
  number = {2}
}

@article{fukushimaNeocognitronSelforganizingNeural1980,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  shorttitle = {Neocognitron},
  author = {Fukushima, Kunihiko},
  date = {1980-04},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  volume = {36},
  pages = {193--202},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00344251},
  url = {http://link.springer.com/10.1007/BF00344251},
  urldate = {2020-10-18},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of selforganization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cells of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\PIQ6RF8J\\Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf},
  langid = {english},
  number = {4}
}

@inproceedings{garcia-nietoEmpiricalComputationQuasioptimal2011,
  title = {Empirical Computation of the Quasi-Optimal Number of Informants in Particle Swarm Optimization},
  author = {García-Nieto, José and Alba, Enrique},
  date = {2011-01-01},
  pages = {147--154},
  doi = {10.1145/2001576.2001597},
  abstract = {In the standard particle swarm optimization (PSO), a new particle's position is generated using two main informant elements: the best position the particle has found so far and the best performer among its neighbors. In fully informed PSO, each particle is influenced by all the remaining ones in the swarm, or by a series of neighbors structured in static topologies (ring, square, or clusters). In this paper, we generalize and analyze the number of informants that take part in the calculation of new particles. Our aim is to discover if a quasi-optimal number of informants exists for a given problem. The experimental results seem to suggest that 6 to 8 informants could provide our PSO with higher chances of success in continuous optimization for well-known benchmarks.},
  eventtitle = {Genetic and {{Evolutionary Computation Conference}}, {{GECCO}}'11},
  file = {C\:\\Users\\fried\\Zotero\\storage\\9RPXBLHU\\García-Nieto and Alba - 2011 - Empirical computation of the quasi-optimal number .pdf}
}

@inproceedings{garcia-nietoWhySixInformants2012,
  title = {Why Six Informants Is Optimal in {{PSO}}},
  booktitle = {Proceedings of the 14th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Garcia-Nieto, José and Alba, Enrique},
  date = {2012-07-07},
  pages = {25--32},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2330163.2330168},
  url = {https://doi.org/10.1145/2330163.2330168},
  urldate = {2020-11-21},
  abstract = {In a previous work, it was empirically shown that certain numbers of informants different from the standard "two" and the expensive "all" may provide the Particle Swarm Optimization (PSO) with new essential information about the search landscape, leading this algorithm to perform more accurately than other existing versions of it. Here, we extend this study by analyzing the internal behavior of PSO from the point of view of the evolvability. Our motivation is to find evidences of why such number of 6+/-2 informant particles, perform better than other neighborhood formulations of PSO. For this task, we have evaluated different combinations of informants for an extensive set of problem functions. Using fitness-distance correlation and fitness-fitness cloud analyses we have tested the accuracy of the resulting landscape characterizations. The results suggest that, in spite of certain deviation to the global optimum, a number of 6 informants in PSO can generate new improved particles for a longer time, even in complex problems with multi-funnel landscapes.},
  isbn = {978-1-4503-1177-9},
  keywords = {fitness-distance correlation,fitness-fitness cloud,fully informed pso,particle swarm optimization},
  series = {{{GECCO}} '12}
}

@online{GoingDeeperEmbedded,
  title = {Going {{Deeper}} with {{Embedded FPGA Platform}} for {{Convolutional Neural Network}} | {{Proceedings}} of the 2016 {{ACM}}/{{SIGDA International Symposium}} on {{Field}}-{{Programmable Gate Arrays}}},
  url = {https://dl.acm.org/doi/abs/10.1145/2847263.2847265},
  urldate = {2020-11-02},
  file = {C\:\\Users\\fried\\Zotero\\storage\\89PUQ9HC\\2847263.html}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {{MIT Press}}
}

@online{GoogleWinsMLPerf,
  title = {Google Wins {{MLPerf}} Benchmark Contest with Fastest {{ML}} Training Supercomputer},
  journaltitle = {Google Cloud Blog},
  url = {https://cloud.google.com/blog/products/ai-machine-learning/google-breaks-ai-performance-records-in-mlperf-with-worlds-fastest-training-supercomputer/},
  urldate = {2020-11-15},
  abstract = {Google set performance records in six out of the eight MLPerf benchmarks at the latest MLPerf benchmark contest},
  file = {C\:\\Users\\fried\\Zotero\\storage\\MSJU7NAZ\\google-breaks-ai-performance-records-in-mlperf-with-worlds-fastest-training-supercomputer.html},
  langid = {english}
}

@online{hanDeepCompressionCompressing2016,
  title = {Deep {{Compression}}: {{Compressing Deep Neural Networks}} with {{Pruning}}, {{Trained Quantization}} and {{Huffman Coding}}},
  shorttitle = {Deep {{Compression}}},
  author = {Han, Song and Mao, Huizi and Dally, William J.},
  date = {2016-02-15},
  url = {http://arxiv.org/abs/1510.00149},
  urldate = {2020-11-06},
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
  archivePrefix = {arXiv},
  eprint = {1510.00149},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\TBRGS48R\\Han et al. - 2016 - Deep Compression Compressing Deep Neural Networks.pdf;C\:\\Users\\fried\\Zotero\\storage\\3GF87AF3\\1510.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@inproceedings{hanEIEEfficientInference2016,
  title = {{{EIE}}: {{Efficient Inference Engine}} on {{Compressed Deep Neural Network}}},
  shorttitle = {{{EIE}}},
  booktitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
  date = {2016-06},
  pages = {243--254},
  publisher = {{IEEE}},
  location = {{Seoul, South Korea}},
  doi = {10.1109/ISCA.2016.30},
  url = {http://ieeexplore.ieee.org/document/7551397/},
  urldate = {2020-11-02},
  abstract = {State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power.},
  eventtitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  file = {C\:\\Users\\fried\\Zotero\\storage\\C6YH7X7H\\Han et al. - 2016 - EIE Efficient Inference Engine on Compressed Deep.pdf},
  isbn = {978-1-4673-8947-1},
  keywords = {Inference acccelerator},
  langid = {english}
}

@online{hanLearningBothWeights2015,
  title = {Learning Both {{Weights}} and {{Connections}} for {{Efficient Neural Networks}}},
  author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
  date = {2015-10-30},
  url = {http://arxiv.org/abs/1506.02626},
  urldate = {2020-10-30},
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9×, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13×, from 138 million to 10.3 million, again with no loss of accuracy.},
  archivePrefix = {arXiv},
  eprint = {1506.02626},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\36J2E2VC\\Han et al. - 2015 - Learning both Weights and Connections for Efficien.pdf},
  keywords = {Compression,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Pruning},
  langid = {english},
  primaryClass = {cs}
}

@online{hanLearningBothWeights2015a,
  title = {Learning Both {{Weights}} and {{Connections}} for {{Efficient Neural Networks}}},
  author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
  date = {2015-10-30},
  url = {http://arxiv.org/abs/1506.02626},
  urldate = {2020-11-13},
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
  archivePrefix = {arXiv},
  eprint = {1506.02626},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\TCVV58GI\\Han et al. - 2015 - Learning both Weights and Connections for Efficien.pdf;C\:\\Users\\fried\\Zotero\\storage\\YHELZTA8\\1506.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{hansonComparingBiasesMinimal,
  title = {Comparing {{Biases}} for {{Minimal Network Construction}} with {{Back}}-{{Propagation}}},
  author = {Hanson, Stephen Jose and Pratt, Lorien Y},
  pages = {9},
  abstract = {Rumelhart (1987). has proposed a method for choosing minimal or "simple" representations during learning in Back-propagation networks. This approach can be used to (a) dynamically select the number of hidden units. (b) construct a representation that is appropriate for the problem and (c) thus improve the generalization ability of Back-propagation networks. The method Rumelhart suggests involves adding penalty terms to the usual error function. In this paper we introduce Rumelhart·s minimal networks idea and compare two possible biases on the weight search space. These biases are compared in both simple counting problems and a speech recognition problem. In general. the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\I8WCHJLJ\\Hanson and Pratt - Comparing Biases for Minimal Network Construction .pdf},
  langid = {english}
}

@article{hassibiSecondOrderDerivatives,
  title = {Second {{Order Derivatives}} for {{Network Pruning}}: {{Optimal Brain Surgeon}}},
  author = {Hassibi, Babak and Stork, David G},
  pages = {8},
  abstract = {We investigate the use of information from all second order derivatives of the error function to perfonn network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Sol1a, 1990], which often remove the wrong weights. OBS permits the pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-I from training data and structural information of the net. OBS permits a 90\%, a 76\%, and a 62\% reduction in weights over backpropagation with weighL decay on three benchmark MONK's problems [Thrun et aI., 1991]. Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg [1987J used 18,000 weights in their NETtalk network, we used OBS to prune a network to just 1560 weights, yielding better generalization.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\PTKYTYB2\\Hassibi and Stork - Second Order Derivatives for Network Pruning Opti.pdf},
  langid = {english}
}

@incollection{heAMCAutoMLModel2018,
  title = {{{AMC}}: {{AutoML}} for {{Model Compression}} and {{Acceleration}} on {{Mobile Devices}}},
  shorttitle = {{{AMC}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  volume = {11211},
  pages = {815--832},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01234-2_48},
  url = {http://link.springer.com/10.1007/978-3-030-01234-2_48},
  urldate = {2020-10-02},
  abstract = {Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-ofthe-art model compression results in a fully automated way without any human efforts. Under 4× FLOPs reduction, we achieved 2.7\% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53× on the GPU (Titan Xp) and 1.95× on an Android phone (Google Pixel 1), with negligible loss of accuracy.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\XZWC8IDP\\He et al. - 2018 - AMC AutoML for Model Compression and Acceleration.pdf},
  isbn = {978-3-030-01233-5 978-3-030-01234-2},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@online{heAMCAutoMLModel2019,
  title = {{{AMC}}: {{AutoML}} for {{Model Compression}} and {{Acceleration}} on {{Mobile Devices}}},
  shorttitle = {{{AMC}}},
  author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  date = {2019-01-15},
  url = {http://arxiv.org/abs/1802.03494},
  urldate = {2020-10-02},
  abstract = {Model compression is an effective technique facilitating the deployment of neural network models on mobile devices that have limited computation resources and a tight power budget. However, conventional model compression techniques [19, 20, 23] use hand-crafted features and require domain experts to explore the large design space trading off model size, speed, and accuracy, which is usually suboptimal and time-consuming. In this paper, we propose Automated Deep Compression (ADC) that leverages reinforcement learning in order to efficiently sample the design space and greatly improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4× FLOPs reduction, we achieved 2.7\% better accuracy than hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet and achieved a 2× reduction in FLOPs, and a speedup of 1.49× on Titan Xp and 1.65× on an Android phone (Samsung Galaxy S7), with negligible loss of accuracy.},
  archivePrefix = {arXiv},
  eprint = {1802.03494},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\KRYGZA2M\\He et al. - 2019 - AMC AutoML for Model Compression and Acceleration.pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  langid = {english},
  primaryClass = {cs}
}

@article{hintonLearningRepresentationsBackpropagating,
  ids = {rumelhartLearningRepresentationsBackpropagating1986},
  title = {Learning Representations by Back-Propagating Errors},
  author = {Hinton, Geoffrey and Rumelhart, David and Williams, Ronald},
  journaltitle = {Nature},
  volume = {323},
  publisher = {{Nature Publishing Group}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\P447BCCW\\rumelhart-1986.pdf;C\:\\Users\\fried\\Zotero\\storage\\VLIAFYL7\\_.pdf;C\:\\Users\\fried\\Zotero\\storage\\GVW959MX\\323533a0.html},
  number = {6088}
}

@inproceedings{hochstetlerEmbeddedDeepLearning2018,
  title = {Embedded {{Deep Learning}} for {{Vehicular Edge Computing}}},
  booktitle = {2018 {{IEEE}}/{{ACM Symposium}} on {{Edge Computing}} ({{SEC}})},
  author = {Hochstetler, Jacob and Padidela, Rahul and Chen, Qi and Yang, Qing and Fu, Song},
  date = {2018-10},
  pages = {341--343},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/SEC.2018.00038},
  url = {https://ieeexplore.ieee.org/document/8567683/},
  urldate = {2020-10-01},
  abstract = {The accuracy of object recognition has been greatly improved due to the rapid development of deep learning, but the deep learning generally requires a lot of training data and the training process is very slow and complex. In this work, an Intel Movidius™ Neural Compute Stick along with Raspberry Pi 3 Model B is used to analyze the objects in the real time images and videos for vehicular edge computing. The results shown in this study tells how the stick performs in conjunction with different operating systems and processing power.},
  eventtitle = {2018 {{IEEE}}/{{ACM Symposium}} on {{Edge Computing}} ({{SEC}})},
  file = {C\:\\Users\\fried\\Zotero\\storage\\E2WQQVRF\\Hochstetler et al. - 2018 - Embedded Deep Learning for Vehicular Edge Computin.pdf},
  isbn = {978-1-5386-9445-9},
  langid = {english}
}

@article{hubelReceptiveFieldsBinocular1962,
  title = {Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex},
  author = {Hubel, D. H. and Wiesel, T. N.},
  date = {1962-01-01},
  journaltitle = {The Journal of Physiology},
  volume = {160},
  pages = {106--154},
  issn = {00223751},
  doi = {10.1113/jphysiol.1962.sp006837},
  url = {http://doi.wiley.com/10.1113/jphysiol.1962.sp006837},
  urldate = {2020-10-15},
  file = {C\:\\Users\\fried\\Zotero\\storage\\3FAJ3L3X\\Hubel and Wiesel - 1962 - Receptive fields, binocular interaction and functi.pdf},
  langid = {english},
  number = {1}
}

@online{ievangelistHowRaiseConsume,
  title = {How to: {{Raise}} and {{Consume Events}}},
  shorttitle = {How To},
  author = {IEvangelist},
  url = {https://docs.microsoft.com/en-us/dotnet/standard/events/how-to-raise-and-consume-events},
  urldate = {2020-10-29},
  abstract = {Raise \& consume events in .NET. See examples that use the EventHandler delegate, the EventHandler delegate, \& a custom delegate.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\EJHPHBVG\\how-to-raise-and-consume-events.html},
  langid = {american}
}

@article{imSparsityOptimizationFramework2004,
  title = {Sparsity: {{Optimization Framework}} for {{Sparse Matrix Kernels}}},
  shorttitle = {Sparsity},
  author = {Im, Eun-Jin and Yelick, Katherine and Vuduc, Richard},
  date = {2004-02},
  journaltitle = {The International Journal of High Performance Computing Applications},
  shortjournal = {The International Journal of High Performance Computing Applications},
  volume = {18},
  pages = {135--158},
  issn = {1094-3420, 1741-2846},
  doi = {10.1177/1094342004041296},
  url = {http://journals.sagepub.com/doi/10.1177/1094342004041296},
  urldate = {2020-10-15},
  abstract = {Sparse matrix-vector multiplication is an important computational kernel that performs poorly on most modern processors due to a low compute-to-memory ratio and irregular memory access patterns. Optimization is difficult because of the complexity of cache-based memory systems and because performance is highly dependent on the nonzero structure of the matrix. The Sparsity system is designed to address these problems by allowing users to automatically build sparse matrix kernels that are tuned to their matrices and machines. Sparsity combines traditional techniques such as loop transformations with data structure transformations and optimization heuristics that are specific to sparse matrices. It provides a novel framework for selecting optimization parameters, such as block size, using a combination of performance models and search.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\LCDXZNIV\\Im et al. - 2004 - Sparsity Optimization Framework for Sparse Matrix.pdf},
  langid = {english},
  number = {1}
}

@online{IntelMyriadVision2017,
  title = {Intel’s {{Myriad X Vision Chip Incorporates Neural Network}}},
  date = {2017-08-30},
  journaltitle = {Electronic Design},
  url = {https://www.electronicdesign.com/industrial-automation/article/21805511/intels-myriad-x-vision-chip-incorporates-neural-network},
  urldate = {2020-11-12},
  abstract = {The company’s latest visual processing unit (VPU) incorporates a hardware neural network along with hardware accelerators.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\2Z6CM8AS\\movidius-myriad-x.png;C\:\\Users\\fried\\Zotero\\storage\\U7YZGYC6\\intels-myriad-x-vision-chip-incorporates-neural-network.html},
  langid = {american}
}

@online{jainCheckmateBreakingMemory2020,
  title = {Checkmate: {{Breaking}} the {{Memory Wall}} with {{Optimal Tensor Rematerialization}}},
  shorttitle = {Checkmate},
  author = {Jain, Paras and Jain, Ajay and Nrusimha, Aniruddha and Gholami, Amir and Abbeel, Pieter and Keutzer, Kurt and Stoica, Ion and Gonzalez, Joseph E.},
  date = {2020-05-14},
  url = {http://arxiv.org/abs/1910.02653},
  urldate = {2020-10-30},
  abstract = {We formalize the problem of trading-off DNN training time and memory requirements as the tensor rematerialization optimization problem, a generalization of prior checkpointing strategies. We introduce Checkmate, a system that solves for optimal rematerialization schedules in reasonable times (under an hour) using off-the-shelf MILP solvers or near-optimal schedules with an approximation algorithm, then uses these schedules to accelerate millions of training iterations. Our method scales to complex, realistic architectures and is hardware-aware through the use of accelerator-specific, profile-based cost models. In addition to reducing training cost, Checkmate enables real-world networks to be trained with up to 5.1× larger input sizes. Checkmate is an open-source project, available at https://github.com/parasj/checkmate.},
  archivePrefix = {arXiv},
  eprint = {1910.02653},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\PESSFWRJ\\Jain et al. - 2020 - Checkmate Breaking the Memory Wall with Optimal T.pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@online{jainOoOVLIWJIT2019,
  title = {The {{OoO VLIW JIT Compiler}} for {{GPU Inference}}},
  author = {Jain, Paras and Mo, Xiangxi and Jain, Ajay and Tumanov, Alexey and Gonzalez, Joseph E. and Stoica, Ion},
  date = {2019-01-30},
  url = {http://arxiv.org/abs/1901.10008},
  urldate = {2020-11-02},
  abstract = {Current trends in Machine Learning (ML) inference on hardware accelerated devices (e.g., GPUs, TPUs) point to alarmingly low utilization. As ML inference is increasingly time-bounded by tight latency SLOs, increasing data parallelism is not an option. The need for better efficiency motivates GPU multiplexing. Furthermore, existing GPU programming abstractions force programmers to micro-manage GPU resources in an early-binding, context-free fashion. We propose a VLIW-inspired Outof-Order (OoO) Just-in-Time (JIT) compiler that coalesces and reorders execution kernels at runtime for throughput-optimal device utilization while satisfying latency SLOs. We quantify the inefficiencies of spaceonly and time-only multiplexing alternatives and demonstrate an achievable 7.7x opportunity gap through spatial coalescing.},
  archivePrefix = {arXiv},
  eprint = {1901.10008},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\CZFZJEMS\\Jain et al. - 2019 - The OoO VLIW JIT Compiler for GPU Inference.pdf},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  langid = {english},
  primaryClass = {cs}
}

@inproceedings{jiangNovelDataTransformation2020,
  ids = {jiangNovelDataTransformation2020a},
  title = {A Novel Data Transformation and Execution Strategy for Accelerating Sparse Matrix Multiplication on {{GPUs}}},
  booktitle = {Proceedings of the 25th {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  author = {Jiang, Peng and Hong, Changwan and Agrawal, Gagan},
  date = {2020-02-19},
  pages = {376--388},
  publisher = {{ACM}},
  location = {{San Diego California}},
  doi = {10.1145/3332466.3374546},
  url = {https://dl.acm.org/doi/10.1145/3332466.3374546},
  urldate = {2020-10-02},
  abstract = {SpMM (multiplication of a sparse matrix and a dense matrix) and SDDMM (sampled dense-dense matrix multiplication) are at the core of many scientific, machine learning, and data mining applications. Because of the irregular memory accesses, the two kernels have poor data locality, and data movement overhead is a bottleneck for their performance. To overcome this issue, previous works have proposed using tiling and data reorganization to enhance data reuse. Despite their success in improving the performance for many sparse matrices, we find that the efficacy of existing techniques largely depends on how the non-zeros are distributed in a sparse matrix. In this work, we propose a novel rowreordering technique to improve data locality for SpMM and SDDMM on GPUs. The goal of such row reordering is to place similar rows close to each other, allowing them to be processed together, and thus providing better temporal locality for the values of the dense matrix. We focus on performing the row-reordering efficiently, by using a hierarchical clustering procedure optimized by locality-sensitive hashing. We also investigate when row-reordering is useful, and what factors the performance gains from our method are correlated to. Experimental evaluation using 1084 sparse matrices from SuiteSparse collection and Network Repository shows that our technique achieves up to 2.91x speedup for SpMM and up to 3.19x speedup for SDDMM against the state-of-the-art alternatives on an Nvidia P100 GPU.},
  eventtitle = {{{PPoPP}} '20: 25th {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\HP8PTGKJ\\Jiang et al. - 2020 - A novel data transformation and execution strategy.pdf;C\:\\Users\\fried\\Zotero\\storage\\ICRVR7T2\\Jiang et al. - 2020 - A novel data transformation and execution strategy.pdf},
  isbn = {978-1-4503-6818-6},
  langid = {english}
}

@online{karelzHttpClientClassSystem,
  title = {{{HttpClient Class}} ({{System}}.{{Net}}.{{Http}})},
  author = {{karelz}},
  url = {https://docs.microsoft.com/en-us/dotnet/api/system.net.http.httpclient},
  urldate = {2020-10-29},
  abstract = {Provides a base class for sending HTTP requests and receiving HTTP responses from a resource identified by a URI.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\C5HPAX2B\\system.net.http.html},
  langid = {american}
}

@online{karelzHttpResponseMessageClassSystem,
  title = {{{HttpResponseMessage Class}} ({{System}}.{{Net}}.{{Http}})},
  author = {{karelz}},
  url = {https://docs.microsoft.com/en-us/dotnet/api/system.net.http.httpresponsemessage},
  urldate = {2020-10-29},
  abstract = {Represents a HTTP response message including the status code and data.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\B4CWAUVZ\\system.net.http.html},
  langid = {american}
}

@inproceedings{kasturiHybridFusionLearning2020,
  title = {Hybrid {{Fusion Learning}}: {{A Hierarchical Learning Model For Distributed Systems}}},
  shorttitle = {Hybrid {{Fusion Learning}}},
  booktitle = {Proceedings of the 4th {{International Workshop}} on {{Embedded}} and {{Mobile Deep Learning}}},
  author = {Kasturi, Anirudh and Ellore, Anish Reddy and Saxena, Paresh and Hota, Chittaranjan},
  date = {2020-09-21},
  pages = {1--6},
  publisher = {{ACM}},
  location = {{London United Kingdom}},
  doi = {10.1145/3410338.3412339},
  url = {https://dl.acm.org/doi/10.1145/3410338.3412339},
  urldate = {2020-10-01},
  abstract = {Federated and fusion learning methods are state-of-the-art distributed learning approaches which enable model training without collecting private data from users. While federated learning involves lower computation cost as compared to fusion learning, the overall communication cost is higher due to a large number of communication rounds between the clients and the server. On the other hand, fusion learning reduces the overall communication cost by sending distributions of features and model parameters using only one communication round but suffers from high computation cost as it needs to find the distributions of features at the client. This paper presents hybrid fusion learning, a system that leverages hierarchical client-edge-cloud architecture and builds a deep learning model by integrating both fusion and federated learning methods. Our proposed approach uses fusion learning between the client and the edge layer to minimise the communication cost whereas it uses federated learning between the edge and the cloud layer to minimise the computation cost. Our results show that the proposed hybrid fusion learning can significantly reduce the total time taken to train the model with a small drop of around 2\% in accuracies as compared to the other two algorithms. Specifically, our results show that fusion and federated learning algorithms take up to 26.28\% and 9.74\% higher average total time to build the model, respectively, than the proposed hybrid fusion learning approach.},
  eventtitle = {{{MobiCom}} '20: {{The}} 26th {{Annual International Conference}} on {{Mobile Computing}} and {{Networking}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\TNSWR3SE\\Kasturi et al. - 2020 - Hybrid Fusion Learning A Hierarchical Learning Mo.pdf},
  isbn = {978-1-4503-8073-7},
  langid = {english}
}

@article{lecunConvolutionalNetworksImages,
  ids = {bengioConvolutionalNetworksImages1997},
  title = {Convolutional {{Networks}} for {{Images}}, {{Speech}}, and {{Time}}-{{Series}}},
  author = {LeCun, Yann and Bengio, Yoshua and Laboratories, T Bell},
  journaltitle = {The handbook of brain theory and neural networks MIT Press},
  pages = {15},
  file = {C\:\\Users\\fried\\Zotero\\storage\\QKTZN3KT\\LeCun et al. - Convolutional Networks for Images, Speech, and Tim.pdf},
  langid = {english}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {521},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  url = {http://www.nature.com/articles/nature14539},
  urldate = {2020-10-15},
  file = {C\:\\Users\\fried\\Zotero\\storage\\WFAGSZVQ\\LeCun et al. - 2015 - Deep learning.pdf},
  langid = {english},
  number = {7553}
}

@incollection{lecunEfficientBackProp2012,
  title = {Efficient {{BackProp}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {LeCun, Yann A. and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
  editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  date = {2012},
  pages = {9--48},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_3},
  url = {https://doi.org/10.1007/978-3-642-35289-8_3},
  urldate = {2020-10-21},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\MNBFPZT3\\LeCun et al. - 2012 - Efficient BackProp.pdf},
  isbn = {978-3-642-35289-8},
  keywords = {Conjugate Gradient,Gradient Descent,Handwritten Digit,Neural Information Processing System,Newton Algorithm},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {Nov./1998},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {86},
  pages = {2278--2324},
  issn = {00189219},
  doi = {10.1109/5.726791},
  url = {http://ieeexplore.ieee.org/document/726791/},
  urldate = {2020-10-18},
  file = {C\:\\Users\\fried\\Zotero\\storage\\73B3Y9MQ\\Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf},
  langid = {english},
  number = {11}
}

@article{lecunOptimalBrainDamage,
  title = {Optimal {{Brain Damage}}},
  author = {LeCun, Yann and Denker, John S and Solla, Sara A},
  pages = {8},
  abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\58AF5J8Q\\LeCun et al. - Optimal Brain Damage.pdf},
  langid = {english}
}

@inproceedings{liangEvolutionaryNeuralAutoML2019,
  ids = {liangEvolutionaryNeuralAutoML2019a},
  title = {Evolutionary Neural {{AutoML}} for Deep Learning},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Liang, Jason and Meyerson, Elliot and Hodjat, Babak and Fink, Dan and Mutch, Karl and Miikkulainen, Risto},
  date = {2019-07-13},
  pages = {401--409},
  publisher = {{ACM}},
  location = {{Prague Czech Republic}},
  doi = {10.1145/3321707.3321721},
  url = {https://dl.acm.org/doi/10.1145/3321707.3321721},
  urldate = {2020-10-01},
  abstract = {Deep neural networks (DNNs) have produced state-of-the-art results in many benchmarks and problem domains. However, the success of DNNs depends on the proper configuration of its architecture and hyperparameters. Such a configuration is difficult and as a result, DNNs are often not used to their full potential. In addition, DNNs in commercial applications often need to satisfy real-world design constraints such as size or number of parameters. To make configuration easier, automatic machine learning (AutoML) systems for deep learning have been developed, focusing mostly on optimization of hyperparameters.},
  eventtitle = {{{GECCO}} '19: {{Genetic}} and {{Evolutionary Computation Conference}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\4P3MN23G\\Liang et al. - 2019 - Evolutionary neural AutoML for deep learning.pdf;C\:\\Users\\fried\\Zotero\\storage\\T9LM58NG\\Liang et al. - 2019 - Evolutionary neural AutoML for deep learning.pdf},
  isbn = {978-1-4503-6111-8},
  langid = {english}
}

@inproceedings{liDetailedDataNeural2019,
  title = {The {{Detailed Data}} on the {{Neural Compute Stick Acceleration Performance}}},
  booktitle = {2019 {{Chinese Automation Congress}} ({{CAC}})},
  author = {Li, Qian and Song, Jiayu and Ning, Jiangbo and Yuan, Jianping},
  date = {2019-11},
  pages = {4959--4962},
  publisher = {{IEEE}},
  location = {{Hangzhou, China}},
  doi = {10.1109/CAC48633.2019.8996841},
  url = {https://ieeexplore.ieee.org/document/8996841/},
  urldate = {2020-10-01},
  abstract = {The four trends such as real-time, intelligence, security and privacy have spawned the rise of edge computing and front-end intelligence. The Neural Computing stick(NCS) can move calculations from the cloud to the terminal, and provide dedicated deep neural network acceleration for terminal AI devices. However, there is currently no detailed data comparison of the NCS acceleration performance. This paper compares the computational speed of convolutional neural networks on the Raspberry Pi in the presence or absence of the NCS. In addition, we further compare the calculated speeds of the CPU, GPU, and NCS through the vehicle model. The result shows that the NCS can achieve 4 to 6 times acceleration in neural networks.},
  eventtitle = {2019 {{Chinese Automation Congress}} ({{CAC}})},
  file = {C\:\\Users\\fried\\Zotero\\storage\\NSEXATJI\\Li et al. - 2019 - The Detailed Data on the Neural Compute Stick Acce.pdf},
  isbn = {978-1-72814-094-0},
  langid = {english}
}

@article{likamwaRedEyeAnalogConvNet,
  title = {{{RedEye}}: {{Analog ConvNet Image Sensor Architecture}} for {{Continuous Mobile Vision}}},
  author = {LiKamWa, Robert and Hou, Yunhui and Gao, Julian and Polansky, Mia and Zhong, Lin},
  pages = {12},
  abstract = {Continuous mobile vision is limited by the inability to efficiently capture image frames and process vision features. This is largely due to the energy burden of analog readout circuitry, data traffic, and intensive computation. To promote efficiency, we shift early vision processing into the analog domain. This results in RedEye, an analog convolutional image sensor that performs layers of a convolutional neural network in the analog domain before quantization. We design RedEye to mitigate analog design complexity, using a modular column-parallel design to promote physical design reuse and algorithmic cyclic reuse. RedEye uses programmable mechanisms to admit noise for tunable energy reduction. Compared to conventional systems, RedEye reports an 85\% reduction in sensor energy, 73\% reduction in cloudlet-based system energy, and a 45\% reduction in computation-based system energy.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\4QK4Y7PS\\LiKamWa et al. - RedEye Analog ConvNet Image Sensor Architecture f.pdf},
  keywords = {Inference acccelerator},
  langid = {english}
}

@article{liLearningIoTEdge2018,
  title = {Learning {{IoT}} in {{Edge}}: {{Deep Learning}} for the {{Internet}} of {{Things}} with {{Edge Computing}}},
  shorttitle = {Learning {{IoT}} in {{Edge}}},
  author = {Li, He and Ota, Kaoru and Dong, Mianxiong},
  date = {2018-01},
  journaltitle = {IEEE Network},
  shortjournal = {IEEE Network},
  volume = {32},
  pages = {96--101},
  issn = {0890-8044, 1558-156X},
  doi = {10.1109/MNET.2018.1700202},
  url = {https://ieeexplore.ieee.org/document/8270639/},
  urldate = {2020-10-01},
  abstract = {Deep learning is a promising approach for extracting accurate information from raw sensor data from IoT devices deployed in complex environments. Because of its multilayer structure, deep learning is also appropriate for the edge computing environment. Therefore, in this article, we first introduce deep learning for IoTs into the edge computing environment. Since existing edge nodes have limited processing capability, we also design a novel offloading strategy to optimize the performance of IoT deep learning applications with edge computing. In the performance evaluation, we test the performance of executing multiple deep learning tasks in an edge computing environment with our strategy. The evaluation results show that our method outperforms other optimization solutions on deep learning for IoT.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\VEY2IB9C\\Li et al. - 2018 - Learning IoT in Edge Deep Learning for the Intern.pdf},
  langid = {english},
  number = {1}
}

@online{liPruningFiltersEfficient2017,
  title = {Pruning {{Filters}} for {{Efficient ConvNets}}},
  author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  date = {2017-03-10},
  url = {http://arxiv.org/abs/1608.08710},
  urldate = {2020-10-30},
  abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34\% and ResNet-110 by up to 38\% on CIFAR10 while regaining close to the original accuracy by retraining the networks.},
  archivePrefix = {arXiv},
  eprint = {1608.08710},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\K96M69NS\\Li et al. - 2017 - Pruning Filters for Efficient ConvNets.pdf},
  keywords = {Compression,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Pruning},
  langid = {english},
  primaryClass = {cs}
}

@online{liuAutoCompressAutomaticDNN2019,
  title = {{{AutoCompress}}: {{An Automatic DNN Structured Pruning Framework}} for {{Ultra}}-{{High Compression Rates}}},
  shorttitle = {{{AutoCompress}}},
  author = {Liu, Ning and Ma, Xiaolong and Xu, Zhiyuan and Wang, Yanzhi and Tang, Jian and Ye, Jieping},
  date = {2019-09-11},
  url = {http://arxiv.org/abs/1907.03141},
  urldate = {2020-10-02},
  abstract = {Structured weight pruning is a representative model compression technique of DNNs to reduce the storage and computation requirements and accelerate inference. An automatic hyperparameter determination process is necessary due to the large number of flexible hyperparameters. This work proposes AutoCompress, an automatic structured pruning framework with the following key performance improvements: (i) effectively incorporate the combination of structured pruning schemes in the automatic process; (ii) adopt the stateof-art ADMM-based structured weight pruning as the core algorithm, and propose an innovative additional purification step for further weight reduction without accuracy loss; and (iii) develop effective heuristic search method enhanced by experience-based guided search, replacing the prior deep reinforcement learning technique which has underlying incompatibility with the target pruning problem. Extensive experiments on CIFAR-10 and ImageNet datasets demonstrate that AutoCompress is the key to achieve ultra-high pruning rates on the number of weights and FLOPs that cannot be achieved before. As an example, AutoCompress outperforms the prior work on automatic model compression by up to 33× in pruning rate (120× reduction in the actual parameter count) under the same accuracy. Significant inference speedup has been observed from the AutoCompress framework on actual measurements on smartphone. We release all models of this work at anonymous link: http://bit.ly/2VZ63dS.},
  archivePrefix = {arXiv},
  eprint = {1907.03141},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\LRHQSIBB\\Liu et al. - 2019 - AutoCompress An Automatic DNN Structured Pruning .pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@article{liUnderstandingErrorPropagation2017,
  title = {Understanding {{Error Propagation}} in {{Deep Learning Neural Network}} ({{DNN}}) {{Accelerators}} and {{Applications}}},
  author = {Li, Guanpeng and Hari, Siva Kumar Sastry and Sullivan, Michael and Tsai, Timothy and Pattabiraman, Karthik and Emer, Joel and Keckler, Stephen W},
  date = {2017},
  pages = {12},
  abstract = {Deep learning neural networks (DNNs) have been successful in solving a wide range of machine learning problems. Specialized hardware accelerators have been proposed to accelerate the execution of DNN algorithms for high-performance and energy efficiency. Recently, they have been deployed in datacenters (potentially for business-critical or industrial applications) and safety-critical systems such as self-driving cars. Soft errors caused by high-energy particles have been increasing in hardware systems, and these can lead to catastrophic failures in DNN systems. Traditional methods for building resilient systems, e.g., Triple Modular Redundancy (TMR), are agnostic of the DNN algorithm and the DNN accelerator’s architecture. Hence, these traditional resilience approaches incur high overheads, which makes them challenging to deploy. In this paper, we experimentally evaluate the resilience characteristics of DNN systems (i.e., DNN software running on specialized accelerators). We find that the error resilience of a DNN system depends on the data types, values, data reuses, and types of layers in the design. Based on our observations, we propose two efficient protection techniques for DNN systems.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\VMIMNXQ6\\Li et al. - 2017 - Understanding Error Propagation in Deep Learning N.pdf},
  langid = {english}
}

@book{lukeEssentialsMetaheuristicsSet2013,
  title = {Essentials of Metaheuristics: A Set of Undergraduate Lecture Notes},
  shorttitle = {Essentials of Metaheuristics},
  author = {Luke, Sean},
  date = {2013},
  edition = {Second edition, online version 2.0},
  publisher = {{lulu.com}},
  location = {{Morrisville, N.C.}},
  annotation = {OCLC: 876159426},
  file = {C\:\\Users\\fried\\Zotero\\storage\\FQNBC27B\\Luke - 2013 - Essentials of metaheuristics a set of undergradua.pdf},
  isbn = {978-1-300-54962-8},
  langid = {english},
  pagetotal = {239}
}

@article{lymDeLTAGPUPerformance,
  title = {{{DeLTA}}: {{GPU Performance Model}} for {{Deep Learning Applications}} with {{In}}-Depth {{Memory System Trafﬁc Analysis}}},
  author = {Lym, Sangkug and Lee, Donghyuk and O’Connor, Mike and Chatterjee, Niladrish and Erez, Mattan},
  pages = {14},
  abstract = {Training convolutional neural networks (CNNs) requires intense compute throughput and high memory bandwidth. Especially, convolution layers account for the majority of execution time of CNN training, and GPUs are commonly used to accelerate these layer workloads. GPU design optimization for efficient CNN training acceleration requires the accurate modeling of how their performance improves when computing and memory resources are increased. We present DeLTA, the first analytical model that accurately estimates the traffic at each GPU memory hierarchy level, while accounting for the complex reuse patterns of a parallel convolution algorithm. We demonstrate that our model is both accurate and robust for different CNNs and GPU architectures. We then show how this model can be used to carefully balance the scaling of different GPU resources for efficient CNN performance improvement.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\GZSGZBTQ\\Lym et al. - DeLTA GPU Performance Model for Deep Learning App.pdf},
  langid = {english}
}

@inproceedings{maMachineLearningEnabled2019,
  title = {Machine Learning Enabled Distributed Mobile Edge Computing Network},
  booktitle = {Proceedings of the 4th {{ACM}}/{{IEEE Symposium}} on {{Edge Computing}}},
  author = {Ma, Junchao and Chang, Hao-Hsuan and Fan, Pingzhi and Liu, Lingjia},
  date = {2019-11-07},
  pages = {350--351},
  publisher = {{ACM}},
  location = {{Arlington Virginia}},
  doi = {10.1145/3318216.3363454},
  url = {https://dl.acm.org/doi/10.1145/3318216.3363454},
  urldate = {2020-10-01},
  abstract = {In this work, we propose to establish a mobile edge computing (MEC) network that considers computation, caching and communication jointly. Depending on the demanding categories, users in the network are partitioned into computation-driven and cachingdriven users, both of which need memory resource to improve their quality of experiences (QoEs). Thus, a memory resource allocation problem is aroused to maximize the performance of the whole network. Due to the fact that the users’ characterization plays an important role to the resource allocation scheme and with the help of machine learning techniques, we propose to study and predict the users’ patterns by distributed learning methods which take the heterogeneity of base station type and users’ mobility, etc into consideration. The proposed machine learning based distributed MEC system can maximize the efficiency of the network by optimizing the resource allocation scheme and perfectly predicting users’ pattern.},
  eventtitle = {{{SEC}} '19: {{The Fourth ACM}}/{{IEEE Symposium}} on {{Edge Computing}}},
  file = {C\:\\Users\\fried\\Zotero\\storage\\YT85YV69\\Ma et al. - 2019 - Machine learning enabled distributed mobile edge c.pdf},
  isbn = {978-1-4503-6733-2},
  langid = {english}
}

@online{maoExploringRegularitySparse2017,
  title = {Exploring the {{Regularity}} of {{Sparse Structure}} in {{Convolutional Neural Networks}}},
  author = {Mao, Huizi and Han, Song and Pool, Jeff and Li, Wenshuo and Liu, Xingyu and Wang, Yu and Dally, William J.},
  date = {2017-06-04},
  url = {http://arxiv.org/abs/1705.08922},
  urldate = {2020-11-17},
  abstract = {Sparsity helps reduce the computational complexity of deep neural networks by skipping zeros. Taking advantage of sparsity is listed as a high priority in next generation DNN accelerators such as TPU. The structure of sparsity, i.e., the granularity of pruning, affects the efficiency of hardware accelerator design as well as the prediction accuracy. Coarse-grained pruning creates regular sparsity patterns, making it more amenable for hardware acceleration but more challenging to maintain the same accuracy. In this paper we quantitatively measure the trade-off between sparsity regularity and prediction accuracy, providing insights in how to maintain accuracy while having more a more structured sparsity pattern. Our experimental results show that coarse-grained pruning can achieve a sparsity ratio similar to unstructured pruning without loss of accuracy. Moreover, due to the index saving effect, coarse-grained pruning is able to obtain a better compression ratio than fine-grained sparsity at the same accuracy threshold. Based on the recent sparse convolutional neural network accelerator (SCNN), our experiments further demonstrate that coarse-grained sparsity saves about 2x the memory references compared to fine-grained sparsity. Since memory reference is more than two orders of magnitude more expensive than arithmetic operations, the regularity of sparse structure leads to more efficient hardware design.},
  archivePrefix = {arXiv},
  eprint = {1705.08922},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\SHWPEPCC\\Mao et al. - 2017 - Exploring the Regularity of Sparse Structure in Co.pdf;C\:\\Users\\fried\\Zotero\\storage\\VE6UDJDW\\1705.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{mayerScalableDeepLearning2020,
  title = {Scalable {{Deep Learning}} on {{Distributed Infrastructures}}: {{Challenges}}, {{Techniques}}, and {{Tools}}},
  shorttitle = {Scalable {{Deep Learning}} on {{Distributed Infrastructures}}},
  author = {Mayer, Ruben and Jacobsen, Hans-Arno},
  date = {2020-05-29},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3363554},
  url = {https://dl.acm.org/doi/10.1145/3363554},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\5IH7WJS9\\Mayer and Jacobsen - 2020 - Scalable Deep Learning on Distributed Infrastructu.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{modasshirDeepNeuralNetworks2018,
  title = {Deep {{Neural Networks}}: {{A Comparison}} on {{Different Computing Platforms}}},
  shorttitle = {Deep {{Neural Networks}}},
  booktitle = {2018 15th {{Conference}} on {{Computer}} and {{Robot Vision}} ({{CRV}})},
  author = {Modasshir, Md and Quattrini Li, Alberto and Rekleitis, Ioannis},
  date = {2018-05},
  pages = {383--389},
  publisher = {{IEEE}},
  location = {{Toronto, ON, Canada}},
  doi = {10.1109/CRV.2018.00060},
  url = {https://ieeexplore.ieee.org/document/8575778/},
  urldate = {2020-10-30},
  abstract = {Deep Neural Networks (DNN) have gained tremendous popularity over the last years for several computer vision tasks, including classification and object detection. Such techniques have been able to achieve human-level performance in many tasks and have produced results of unprecedented accuracy. As DNNs have intense computational requirements in the majority of applications, they utilize a cluster of computers or a cutting edge Graphical Processing Unit (GPU), often having excessive power consumption and generating a lot of heat. In many robotics applications the above requirements prove to be a challenge, as there is limited power on-board and heat dissipation is always a problem. In particular in underwater robotics with limited space, the above two requirements have been proven prohibitive. As first of this kind, this paper aims at analyzing and comparing the performance of several stateof-the-art DNNs on different platforms. With a focus on the underwater domain, the capabilities of the Jetson TX2 from NVIDIA and the Neural Compute Stick from Intel are of particular interest. Experiments on standard datasets show how different platforms are usable on an actual robotic system, providing insights on the current state-of-the-art embedded systems. Based on such results, we propose some guidelines in choosing the appropriate platform and network architecture for a robotic system.},
  eventtitle = {2018 15th {{Conference}} on {{Computer}} and {{Robot Vision}} ({{CRV}})},
  file = {C\:\\Users\\fried\\Zotero\\storage\\3UFKXRTW\\Modasshir et al. - 2018 - Deep Neural Networks A Comparison on Different Co.pdf},
  isbn = {978-1-5386-6481-0},
  langid = {english}
}

@online{ModuleTfKeras,
  title = {Module: Tf.Keras.Layers | {{TensorFlow Core}} v2.3.0},
  shorttitle = {Module},
  journaltitle = {TensorFlow},
  url = {https://www.tensorflow.org/api_docs/python/tf/keras/layers},
  urldate = {2020-11-22},
  abstract = {Keras layers API.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\PJSD6PM2\\layers.html},
  langid = {english}
}

@article{ningDeepReinforcementLearning2019,
  title = {Deep {{Reinforcement Learning}} for {{Vehicular Edge Computing}}: {{An Intelligent Offloading System}}},
  shorttitle = {Deep {{Reinforcement Learning}} for {{Vehicular Edge Computing}}},
  author = {Ning, Zhaolong and Dong, Peiran and Wang, Xiaojie and Rodrigues, Joel J. P. C. and Xia, Feng},
  date = {2019-12-14},
  journaltitle = {ACM Transactions on Intelligent Systems and Technology},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  volume = {10},
  pages = {1--24},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/3317572},
  url = {https://dl.acm.org/doi/10.1145/3317572},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\YNBRPL98\\Ning et al. - 2019 - Deep Reinforcement Learning for Vehicular Edge Com.pdf},
  langid = {english},
  number = {6}
}

@article{nowlanEnhancedKnowledgeDistillation,
  title = {Enhanced {{Knowledge Distillation}} for {{Neural Network Accelerators}}},
  author = {Nowlan, Andrew},
  pages = {92},
  abstract = {Recent breakthroughs in computer vision can be attributed to advances in deep learning, access to large labelled image datasets and intelligent utilisation of available hardware. Increasing demand for computer vision solutions capable of being deployed on resource constrained devices has driven a huge amount of research in both model compression and hardware design. In recent years, a new class of hardware has emerged to satisfy this demand. In particular, accelerator devices such as the Intel Movidius Myriad X VPU and the Google Coral Edge TPU have been designed specifically to accommodate deep learning workloads with reduced power requirements, providing a state-of-the-art trade off between compute performance and power consumption. Offloading deep learning inference workloads to these accelerators enables low power single board computers to run computer vision applications in real time. Even on more conventional computing platforms, these accelerators are a competitive choice of co-processor compared with typical GPUs due to their superior performance per watt. However, many state-of-theart classification models have an enormous number of parameters, making them highly computationally and resource intensive. As such, very large models are not supported by these memory constrained accelerators.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\UBDA8B3C\\Nowlan - Enhanced Knowledge Distillation for Neural Network.pdf},
  langid = {english}
}

@article{parasharSCNNAcceleratorCompressedsparse2017,
  title = {{{SCNN}}: {{An Accelerator}} for {{Compressed}}-Sparse  {{Convolutional Neural Networks}}},
  author = {Parashar, Angshuman and Rhu, Minsoo and Mukkara, Anurag and Puglielli, Antonio and Venkatesan, Rangharajan and Khailany, Brucek and Emer, Joel and Keckler, Stephen W and Dally, William J},
  date = {2017},
  pages = {14},
  abstract = {Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and extreme energy efficiency are critical for deployments of CNNs, especially in mobile platforms such as autonomous vehicles, cameras, and electronic personal assistants. This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efficiency by exploiting the zero-valued weights that stem from network pruning during training and zero-valued activations that arise from the common ReLU operator. Specifically, SCNN employs a novel dataflow that enables maintaining the sparse weights and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore, the SCNN dataflow facilitates efficient delivery of those weights and activations to a multiplier array, where they are extensively reused; product accumulation is performed in a novel accumulator array. On contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7× and 2.3×, respectively, over a comparably provisioned dense CNN accelerator.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\VSPA68F3\\Parashar et al. - 2017 - SCNN An Accelerator for Compressed-sparse  Convol.pdf},
  langid = {english}
}

@article{paulinoImprovingPerformanceEnergy2020,
  title = {Improving {{Performance}} and {{Energy Consumption}} in {{Embedded Systems}} via {{Binary Acceleration}}: {{A Survey}}},
  shorttitle = {Improving {{Performance}} and {{Energy Consumption}} in {{Embedded Systems}} via {{Binary Acceleration}}},
  author = {Paulino, Nuno and Ferreira, João Canas and Cardoso, João M. P.},
  date = {2020-05-29},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3369764},
  url = {https://dl.acm.org/doi/10.1145/3369764},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\8MIBZ5XX\\Paulino et al. - 2020 - Improving Performance and Energy Consumption in Em.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{pesterObjectDetectionRaspberry2019,
  title = {Object Detection with {{Raspberry Pi3}} and {{Movidius Neural Network Stick}}},
  booktitle = {2019 5th {{Experiment International Conference}} (Exp.at'19)},
  author = {Pester, Andreas and Schrittesser, Michael},
  date = {2019-06},
  pages = {326--330},
  publisher = {{IEEE}},
  location = {{Funchal (Madeira Island), Portugal}},
  doi = {10.1109/EXPAT.2019.8876583},
  url = {https://ieeexplore.ieee.org/document/8876583/},
  urldate = {2020-10-01},
  abstract = {Object detection and classification is an increasingly important field of research in machine learning. Currently, powerful GPUs (Graphics Processing Units) are used to perform the computation-intensive operations in the shortest possible computing time. However, these systems are associated with high costs. In this paper a system for object detection and classification is developed, which gets by with less resources. This should minimize the costs while keeping the performance acceptable for the target application. To keep the costs low, a Raspberry Pi3 is used as development platform in connection with a Movidius stick for the outsourcing of the ANN. After explaining the theoretical basics of object detection and ANNs, this paper shows the implementation process of the selected hardware and software. For the evaluation of this system the algorithms YOLO and MobileNet are used and pre-trained models are used as basis. Based on the MSCOCO data set, both the quality of the object classification and the computing time are evaluated.},
  eventtitle = {2019 5th {{Experiment Conference}} (Exp.at'19)},
  file = {C\:\\Users\\fried\\Zotero\\storage\\XZINHH5P\\Pester and Schrittesser - 2019 - Object detection with Raspberry Pi3 and Movidius N.pdf},
  isbn = {978-1-72813-637-0},
  langid = {english}
}

@article{pouyanfarSurveyDeepLearning2019,
  title = {A {{Survey}} on {{Deep Learning}}: {{Algorithms}}, {{Techniques}}, and {{Applications}}},
  shorttitle = {A {{Survey}} on {{Deep Learning}}},
  author = {Pouyanfar, Samira and Sadiq, Saad and Yan, Yilin and Tian, Haiman and Tao, Yudong and Reyes, Maria Presa and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, S. S.},
  date = {2019-01-23},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {51},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3234150},
  url = {https://dl.acm.org/doi/10.1145/3234150},
  urldate = {2020-10-15},
  file = {C\:\\Users\\fried\\Zotero\\storage\\J9UMB6UE\\Pouyanfar et al. - 2019 - A Survey on Deep Learning Algorithms, Techniques,.pdf},
  langid = {english},
  number = {5}
}

@online{PruningNeuralNetwork,
  title = {Pruning - {{Neural Network Distiller}}},
  url = {https://intellabs.github.io/distiller/pruning.html},
  urldate = {2020-10-30},
  file = {C\:\\Users\\fried\\Zotero\\storage\\7G9R6YEL\\pruning.html}
}

@inproceedings{qiuGoingDeeperEmbedded2016,
  title = {Going {{Deeper}} with {{Embedded FPGA Platform}} for {{Convolutional Neural Network}}},
  booktitle = {Proceedings of the 2016 {{ACM}}/{{SIGDA International Symposium}} on {{Field}}-{{Programmable Gate Arrays}}},
  author = {Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen and Wang, Yu and Yang, Huazhong},
  date = {2016-02-21},
  pages = {26--35},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2847263.2847265},
  url = {https://doi.org/10.1145/2847263.2847265},
  urldate = {2020-11-02},
  abstract = {In recent years, convolutional neural network (CNN) based methods have achieved great success in a large number of applications and have been among the most powerful and widely used techniques in computer vision. However, CNN-based methods are com-putational-intensive and resource-consuming, and thus are hard to be integrated into embedded systems such as smart phones, smart glasses, and robots. FPGA is one of the most promising platforms for accelerating CNN, but the limited bandwidth and on-chip memory size limit the performance of FPGA accelerator for CNN. In this paper, we go deeper with the embedded FPGA platform on accelerating CNNs and propose a CNN accelerator design on embedded FPGA for Image-Net large-scale image classification. We first present an in-depth analysis of state-of-the-art CNN models and show that Convolutional layers are computational-centric and Fully-Connected layers are memory-centric. Then the dynamic-precision data quantization method and a convolver design that is efficient for all layer types in CNN are proposed to improve the bandwidth and resource utilization. Results show that only 0.4\% accuracy loss is introduced by our data quantization flow for the very deep VGG16 model when 8/4-bit quantization is used. A data arrangement method is proposed to further ensure a high utilization of the external memory bandwidth. Finally, a state-of-the-art CNN, VGG16-SVD, is implemented on an embedded FPGA platform as a case study. VGG16-SVD is the largest and most accurate network that has been implemented on FPGA end-to-end so far. The system on Xilinx Zynq ZC706 board achieves a frame rate at 4.45 fps with the top-5 accuracy of 86.66\% using 16-bit quantization. The average performance of convolutional layers and the full CNN is 187.8 GOP/s and 137.0 GOP/s under 150MHz working frequency, which outperform previous approaches significantly.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\5XRQHPYI\\Qiu et al. - 2016 - Going Deeper with Embedded FPGA Platform for Convo.pdf;C\:\\Users\\fried\\Zotero\\storage\\A3S89CXX\\Qiu et al. - 2016 - Going Deeper with Embedded FPGA Platform for Convo.pdf},
  isbn = {978-1-4503-3856-1},
  keywords = {bandwidth utilization,convolutional neural network (cnn),dynamic-precision data quantization,embedded fpga},
  series = {{{FPGA}} '16}
}

@inproceedings{qiuGoingDeeperEmbedded2016a,
  title = {Going {{Deeper}} with {{Embedded FPGA Platform}} for {{Convolutional Neural Network}}},
  booktitle = {Proceedings of the 2016 {{ACM}}/{{SIGDA International Symposium}} on {{Field}}-{{Programmable Gate Arrays}} - {{FPGA}} '16},
  author = {Qiu, Jiantao and Song, Sen and Wang, Yu and Yang, Huazhong and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi},
  date = {2016},
  pages = {26--35},
  publisher = {{ACM Press}},
  location = {{Monterey, California, USA}},
  doi = {10.1145/2847263.2847265},
  url = {http://dl.acm.org/citation.cfm?doid=2847263.2847265},
  urldate = {2020-11-02},
  abstract = {In recent years, Convolutional Neural Network (CNN) based methods have achieved great success in a large number of applications and have been among the most powerful and widely used techniques in computer vision. However, CNN-based methods are computational-intensive and resource-consuming, and thus are hard to be integrated into embedded systems such as smart phones, smart glasses, and robots. FPGA is one of the most promising platforms for accelerating CNN, but the limited bandwidth and on-chip memory size limit the performance of FPGA accelerator for CNN.},
  eventtitle = {The 2016 {{ACM}}/{{SIGDA International Symposium}}},
  isbn = {978-1-4503-3856-1},
  langid = {english}
}

@inproceedings{raduPerformanceAwareConvolutional2019,
  title = {Performance {{Aware Convolutional Neural Network Channel Pruning}} for {{Embedded GPUs}}},
  booktitle = {2019 {{IEEE International Symposium}} on {{Workload Characterization}} ({{IISWC}})},
  author = {Radu, V. and Kaszyk, K. and Wen, Y. and Turner, J. and Cano, J. and Crowley, E. J. and Franke, B. and Storkey, A. and O'Boyle, M.},
  date = {2019-11},
  pages = {24--34},
  doi = {10.1109/IISWC47752.2019.9042000},
  abstract = {Convolutional Neural Networks (CNN) are becoming a common presence in many applications and services, due to their superior recognition accuracy. They are increasingly being used on mobile devices, many times just by porting large models designed for server space, although several model compression techniques have been considered. One model compression technique intended to reduce computations is channel pruning. Mobile and embedded systems now have GPUs which are ideal for the parallel computations of neural networks and for their lower energy cost per operation. Specialized libraries perform these neural network computations through highly optimized routines. As we find in our experiments, these libraries are optimized for the most common network shapes, making uninstructed channel pruning inefficient. We evaluate higher level libraries, which analyze the input characteristics of a convolutional layer, based on which they produce optimized OpenCL (Arm Compute Library and TVM) and CUDA (cuDNN) code. However, in reality, these characteristics and subsequent choices intended for optimization can have the opposite effect. We show that a reduction in the number of convolutional channels, pruning 12\% of the initial size, is in some cases detrimental to performance, leading to 2× slowdown. On the other hand, we also find examples where performance-aware pruning achieves the intended results, with performance speedups of 3× with cuDNN and above 10× with Arm Compute Library and TVM. Our findings expose the need for hardware-instructed neural network pruning.},
  eventtitle = {2019 {{IEEE International Symposium}} on {{Workload Characterization}} ({{IISWC}})},
  file = {C\:\\Users\\fried\\Zotero\\storage\\YVWWIQWX\\Radu et al. - 2019 - Performance Aware Convolutional Neural Network Cha.pdf;C\:\\Users\\fried\\Zotero\\storage\\NCHTSHJT\\9042000.html},
  keywords = {Arm Compute Library,channel pruning,convolutional channels,convolutional layer,convolutional neural nets,convolutional neural network channel pruning,convolutional neural networks,data compression,embedded GPU,embedded GPUs,graphics processing units,hardware-instructed neural network pruning,mobile devices,model compression technique,multiprocessing systems,neural network computations,OpenCL,parallel architectures,parallel computations,performance-aware pruning,software libraries}
}

@inproceedings{reagenMinervaEnablingLowPower2016,
  title = {Minerva: {{Enabling Low}}-{{Power}}, {{Highly}}-{{Accurate Deep Neural Network Accelerators}}},
  shorttitle = {Minerva},
  booktitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Reagen, B. and Whatmough, P. and Adolf, R. and Rama, S. and Lee, H. and Lee, S. K. and Hernández-Lobato, J. M. and Wei, G. and Brooks, D.},
  date = {2016-06},
  pages = {267--278},
  issn = {1063-6897},
  doi = {10.1109/ISCA.2016.32},
  abstract = {The continued success of Deep Neural Networks (DNNs) in classification tasks has sparked a trend of accelerating their execution with specialized hardware. While published designs easily give an order of magnitude improvement over general-purpose hardware, few look beyond an initial implementation. This paper presents Minerva, a highly automated co-design approach across the algorithm, architecture, and circuit levels to optimize DNN hardware accelerators. Compared to an established fixed-point accelerator baseline, we show that fine-grained, heterogeneous datatype optimization reduces power by 1.5×; aggressive, inline predication and pruning of small activity values further reduces power by 2.0×; and active hardware fault detection coupled with domain-aware error mitigation eliminates an additional 2.7× through lowering SRAM voltages. Across five datasets, these optimizations provide a collective average of 8.1× power reduction over an accelerator baseline without compromising DNN model accuracy. Minerva enables highly accurate, ultra-low power DNN accelerators (in the range of tens of milliwatts), making it feasible to deploy DNNs in power-constrained IoT and mobile devices.},
  eventtitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  file = {C\:\\Users\\fried\\Zotero\\storage\\K5LPKKLD\\reagen_isca16.pdf;C\:\\Users\\fried\\Zotero\\storage\\TB8J8SKG\\7551399.html},
  keywords = {active hardware fault detection,automated codesign,Circuit faults,classification tasks,deep neural network accelerators,deep neural networks,DNN hardware accelerators,DNN model accuracy,domain-aware error mitigation,fixed-point accelerator baseline,general-purpose hardware,Hardware,heterogeneous datatype optimization,Inference acccelerator,inline predication,Integrated circuit modeling,Libraries,magnitude improvement,Minerva,mobile devices,neural nets,Optimization,power-constrained IoT,Random access memory,small activity values,Space exploration,specialized hardware,SRAM voltages,ultra-low power DNN accelerators}
}

@inproceedings{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-06},
  pages = {779--788},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.91},
  url = {http://ieeexplore.ieee.org/document/7780460/},
  urldate = {2020-10-30},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {C\:\\Users\\fried\\Zotero\\storage\\6JRIR2WX\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf},
  isbn = {978-1-4673-8851-1},
  langid = {english}
}

@online{rhuVDNNVirtualizedDeep2016,
  title = {{{vDNN}}: {{Virtualized Deep Neural Networks}} for {{Scalable}}, {{Memory}}-{{Efficient Neural Network Design}}},
  shorttitle = {{{vDNN}}},
  author = {Rhu, Minsoo and Gimelshein, Natalia and Clemons, Jason and Zulfiqar, Arslan and Keckler, Stephen W.},
  date = {2016-07-28},
  url = {http://arxiv.org/abs/1602.08124},
  urldate = {2020-10-30},
  abstract = {The most widely used machine learning frameworks require users to carefully tune their memory usage so that the deep neural network (DNN) fits into the DRAM capacity of a GPU. This restriction hampers a researcher’s flexibility to study different machine learning algorithms, forcing them to either use a less desirable network architecture or parallelize the processing across multiple GPUs. We propose a runtime memory manager that virtualizes the memory usage of DNNs such that both GPU and CPU memory can simultaneously be utilized for training larger DNNs. Our virtualized DNN (vDNN) reduces the average GPU memory usage of AlexNet by up to 89\%, OverFeat by 91\%, and GoogLeNet by 95\%, a significant reduction in memory requirements of DNNs. Similar experiments on VGG-16, one of the deepest and memory hungry DNNs to date, demonstrate the memory-efficiency of our proposal. vDNN enables VGG-16 with batch size 256 (requiring 28 GB of memory) to be trained on a single NVIDIA Titan X GPU card containing 12 GB of memory, with 18\% performance loss compared to a hypothetical, oracular GPU with enough memory to hold the entire DNN.},
  archivePrefix = {arXiv},
  eprint = {1602.08124},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\3YKI6TI4\\Rhu et al. - 2016 - vDNN Virtualized Deep Neural Networks for Scalabl.pdf},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  langid = {english},
  primaryClass = {cs}
}

@book{rightsFreeSpeechJournal,
  title = {The {{Free Speech Journal}}, {{Issue}} 5(1997)},
  author = {Rights, Published All},
  abstract = {This paper presents new methods for training large neural networks for phoneme  probability estimation. An architecture combining time-delay windows and  recurrent connections is used to capture the important dynamic information of the  speech signal. Because the number of connections in a fully connected recurrent  network grows super-linear with the number of hidden units, schemes for sparse  connection and connection pruning are explored. It is found that sparsely  connected networks outperform their fully connected counterparts with an equal  number of connections. The implementation of the combined architecture and  training scheme is described in detail. The networks are evaluated in a hybrid  HMM/ANN system for phoneme recognition on the TIMIT database, and for  word recognition on the WAXHOLM database. The achieved phone error-rate,  27.8\%, for the standard 39 phoneme set on the core test-set of the TIMIT database  is in the range of the lowest reported. All training and simulation software used is  made freely available by the author, and detailed information about the software  and the training process is given in an Appendix.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\Q5WZ2PCB\\Rights - The Free Speech Journal, Issue 5(1997).pdf;C\:\\Users\\fried\\Zotero\\storage\\NBFTFBTP\\download.html;C\:\\Users\\fried\\Zotero\\storage\\WUWTF78T\\download.html}
}

@inproceedings{rogersScalingBandwidthWall2009,
  title = {Scaling the Bandwidth Wall: Challenges in and Avenues for {{CMP}} Scaling},
  shorttitle = {Scaling the Bandwidth Wall},
  booktitle = {Proceedings of the 36th Annual International Symposium on {{Computer}} Architecture},
  author = {Rogers, Brian M. and Krishna, Anil and Bell, Gordon B. and Vu, Ken and Jiang, Xiaowei and Solihin, Yan},
  date = {2009-06-20},
  pages = {371--382},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1555754.1555801},
  url = {https://doi.org/10.1145/1555754.1555801},
  urldate = {2020-11-02},
  abstract = {As transistor density continues to grow at an exponential rate in accordance to Moore's law, the goal for many Chip Multi-Processor (CMP) systems is to scale the number of on-chip cores proportionally. Unfortunately, off-chip memory bandwidth capacity is projected to grow slowly compared to the desired growth in the number of cores. This creates a situation in which each core will have a decreasing amount of off-chip bandwidth that it can use to load its data from off-chip memory. The situation in which off-chip bandwidth is becoming a performance and throughput bottleneck is referred to as the bandwidth wall problem. In this study, we seek to answer two questions: (1) to what extent does the bandwidth wall problem restrict future multicore scaling, and (2) to what extent are various bandwidth conservation techniques able to mitigate this problem. To address them, we develop a simple but powerful analytical model to predict the number of on-chip cores that a CMP can support given a limited growth in memory traffic capacity. We find that the bandwidth wall can severely limit core scaling. When starting with a balanced 8-core CMP, in four technology generations the number of cores can only scale to 24, as opposed to 128 cores under proportional scaling, without increasing the memory traffic requirement. We find that various individual bandwidth conservation techniques we evaluate have a wide ranging impact on core scaling, and when combined together, these techniques have the potential to enable super-proportional core scaling for up to 4 technology generations.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\CXZF7AAH\\Rogers et al. - 2009 - Scaling the bandwidth wall challenges in and aven.pdf},
  isbn = {978-1-60558-526-0},
  keywords = {analytical model,chip multi-processor,memory bandwidth},
  series = {{{ISCA}} '09}
}

@article{schmidhuberDeepLearningNeural2015,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  shorttitle = {Deep Learning in Neural Networks},
  author = {Schmidhuber, Jürgen},
  date = {2015-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {61},
  pages = {85--117},
  issn = {08936080},
  doi = {10.1016/j.neunet.2014.09.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608014002135},
  urldate = {2020-10-21},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\88EUTK6A\\Schmidhuber - 2015 - Deep learning in neural networks An overview.pdf},
  langid = {english}
}

@incollection{scholkopfEfficientLearningSparse2007,
  title = {Efficient {{Learning}} of {{Sparse Representations}} with an {{Energy}}-{{Based Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  editor = {Schölkopf, Bernhard and Platt, John and Hofmann, Thomas},
  date = {2007},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/7503.003.0147},
  url = {https://direct.mit.edu/books/book/3168/chapter/87540/efficient-learning-of-sparse-representations-with},
  urldate = {2020-10-15},
  abstract = {We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces “stroke detectors” when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\NFMWP2YL\\Schölkopf et al. - 2007 - Efficient Learning of Sparse Representations with .pdf},
  isbn = {978-0-262-25691-9},
  langid = {english}
}

@article{shafieeISAACConvolutionalNeural2016,
  title = {{{ISAAC}}: A Convolutional Neural Network Accelerator with in-Situ Analog Arithmetic in Crossbars},
  shorttitle = {{{ISAAC}}},
  author = {Shafiee, Ali and Nag, Anirban and Muralimanohar, Naveen and Balasubramonian, Rajeev and Strachan, John Paul and Hu, Miao and Williams, R. Stanley and Srikumar, Vivek},
  date = {2016-06-18},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  shortjournal = {SIGARCH Comput. Archit. News},
  volume = {44},
  pages = {14--26},
  issn = {0163-5964},
  doi = {10.1145/3007787.3001139},
  url = {https://doi.org/10.1145/3007787.3001139},
  urldate = {2020-11-02},
  abstract = {A number of recent efforts have attempted to design accelerators for popular machine learning algorithms, such as those involving convolutional and deep neural networks (CNNs and DNNs). These algorithms typically involve a large number of multiply-accumulate (dot-product) operations. A recent project, DaDianNao, adopts a near data processing approach, where a specialized neural functional unit performs all the digital arithmetic operations and receives input weights from adjacent eDRAM banks. This work explores an in-situ processing approach, where memristor crossbar arrays not only store input weights, but are also used to perform dot-product operations in an analog manner. While the use of crossbar memory as an analog dot-product engine is well known, no prior work has designed or characterized a full-fledged accelerator based on crossbars. In particular, our work makes the following contributions: (i) We design a pipelined architecture, with some crossbars dedicated for each neural network layer, and eDRAM buffers that aggregate data between pipeline stages. (ii) We define new data encoding techniques that are amenable to analog computations and that can reduce the high overheads of analog-to-digital conversion (ADC). (iii) We define the many supporting digital components required in an analog CNN accelerator and carry out a design space exploration to identify the best balance of memristor storage/compute, ADCs, and eDRAM storage on a chip. On a suite of CNN and DNN workloads, the proposed ISAAC architecture yields improvements of 14.8×, 5.5×, and 7.5× in throughput, energy, and computational density (respectively), relative to the state-of-the-art DaDianNao architecture.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\E2JKLPC5\\Shafiee et al. - 2016 - ISAAC a convolutional neural network accelerator .pdf},
  keywords = {accelerator,analog,CNN,DNN,Inference acccelerator,memristor,neural},
  number = {3}
}

@article{shiBenchmarkingStateoftheArtDeep,
  title = {Benchmarking {{State}}-of-the-{{Art Deep Learning Software Tools}}},
  author = {Shi, Shaohuai and Wang, Qiang and Xu, Pengfei and Chu, Xiaowen},
  pages = {6},
  abstract = {Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training and inference time. However, different tools exhibit different features and running performance when they train different types of deep networks on different hardware platforms, making it difficult for end users to select an appropriate pair of software and hardware. In this paper, we present our attempt to benchmark several state-ofthe-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We focus on evaluating the running time performance (i.e., speed) of these tools with three popular types of neural networks on two representative CPU platforms and three representative GPU platforms. Our contribution is two-fold. First, for end users of deep learning software tools, our benchmarking results can serve as a reference to selecting appropriate hardware platforms and software tools. Second, for developers of deep learning software tools, our in-depth analysis points out possible future directions to further optimize the running performance.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\KX2R35DW\\Shi et al. - Benchmarking State-of-the-Art Deep Learning Softwa.pdf},
  langid = {english}
}

@online{StdResultResult,
  title = {Std::Result::{{Result}} - {{Rust}}},
  url = {https://doc.rust-lang.org/std/result/enum.Result.html#method.unwrap},
  urldate = {2020-10-29},
  file = {C\:\\Users\\fried\\Zotero\\storage\\EBHVTF5K\\enum.Result.html}
}

@online{stromPhonemeProbabilityEstimation1997,
  title = {Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks},
  author = {Strom, N.},
  date = {1997},
  journaltitle = {undefined},
  url = {/paper/Phoneme-probability-estimation-with-dynamic-neural-Strom/a9392b9299972452ea6fbc3c605f76bb1e21ae42},
  urldate = {2020-11-13},
  abstract = {This paper presents new methods for training large neural networks for phoneme probabilit y estimation. An architecture combining time-delay windows and recurrent connections is used to capture the important dynamic information of the speech signal. Because the number of connections in a fully connected recurrent network grows super-linear with the number of hidden units, schemes for sparse connection and connection pruning are explored. It is found that sparsely connected networks outperform their fully connected counterparts with an equal number of connections. The implementation of the combined architecture and training scheme is described in detail . The networks are evaluated in a hybrid HMM/ANN system for phoneme recognition on the TIMIT database, and for word recognition on the WAXHOLM database. The achieved phone error-rate, 27.8\%, for the standard 39 phoneme set on the core test-set of the TIMIT database is in the range of the lowest reported. All training and simulation software used is made freely available by the author, and detailed information about the software and the training process is given in an Appendix. Nikko Ström, Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks 2 Table of contents Abstract...........................................................................................................................1},
  file = {C\:\\Users\\fried\\Zotero\\storage\\KNAVNR24\\a9392b9299972452ea6fbc3c605f76bb1e21ae42.html},
  langid = {english}
}

@article{szeEfficientProcessingDeep2017,
  title = {Efficient {{Processing}} of {{Deep Neural Networks}}: {{A Tutorial}} and {{Survey}}},
  shorttitle = {Efficient {{Processing}} of {{Deep Neural Networks}}},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
  date = {2017-12},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {105},
  pages = {2295--2329},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2017.2761740},
  url = {http://ieeexplore.ieee.org/document/8114708/},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\NP4DHPT5\\Sze et al. - 2017 - Efficient Processing of Deep Neural Networks A Tu.pdf},
  langid = {english},
  number = {12}
}

@online{teamKerasDocumentationModel,
  title = {Keras Documentation: {{The Model}} Class},
  shorttitle = {Keras Documentation},
  author = {Team, Keras},
  url = {https://keras.io/api/models/model/#model-class},
  urldate = {2020-11-22},
  abstract = {Keras documentation},
  file = {C\:\\Users\\fried\\Zotero\\storage\\ZHVXYQ7T\\model.html},
  langid = {english}
}

@online{TfKerasModel,
  title = {Tf.Keras.{{Model}} | {{TensorFlow Core}} v2.3.0},
  journaltitle = {TensorFlow},
  url = {https://www.tensorflow.org/api_docs/python/tf/keras/Model},
  urldate = {2020-11-22},
  abstract = {Model groups layers into an object with training and inference features.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\QIZ7AI7Y\\Model.html},
  langid = {english}
}

@article{thierry-miegHowFundamentalConcepts,
  title = {How the Fundamental Concepts of Mathematics and Physics Explain Deep Learning.},
  author = {Thierry-Mieg, Jean},
  pages = {16},
  abstract = {Starting from the Fermat’s principle of least action, which governs classical and quantum mechanics and from the theory of exterior differential forms, which governs the geometry of curved manifolds, we show how to derive the equations governing neural networks in an intrinsic, coordinate invariant way, where the loss function plays the role of the Hamitonian. To be covariant, these equations imply a layer metric which is instrumental in pretraining and explains the role of conjugation when using complex numbers. The differential formalism also clarifies the relation of the gradient descent optimizer with Aristotelian and Newtonian mechanics and why large learning steps break the logic of the linearization procedure. We hope that this formal presentation of the differential geometry of neural networks will encourage some physicists to dive into deep learning, and reciprocally, that the specialists of deep learning will better appreciate the close interconnection of their subject with the foundations of classical and quantum field theory.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\42KGY4LU\\Thierry-Mieg - How the fundamental concepts of mathematics and ph.pdf},
  langid = {english}
}

@inproceedings{tiwariNCSBasedUltra2019,
  title = {{{NCS}} Based Ultra Low Power Optimized Machine Learning Techniques for Image Classification},
  booktitle = {2019 {{IEEE Region}} 10 {{Symposium}} ({{TENSYMP}})},
  author = {Tiwari, Naman and Mondal, Koushik},
  date = {2019-06},
  pages = {750--753},
  publisher = {{IEEE}},
  location = {{Kolkata, India}},
  doi = {10.1109/TENSYMP46218.2019.8971238},
  url = {https://ieeexplore.ieee.org/document/8971238/},
  urldate = {2020-10-01},
  abstract = {In recent years there has been an extensive development in the field of convolutional neural network-based image classification because of the human-like inference results obtained, but these massive networks are resource intensive and have high memory and computational requirements. Intel’s Neural Compute Stick brings real time inference, prototyping and deployment of these DNNs to the network edge. In this paper we will discuss the development of a model for classification of book cover images into genres, and subsequently compiling the trained model for use with the Neural Compute Stick, so as to receive the optimized results in constrained environments thus ultimately leading to a system to judge a book by its cover which can be used even within a low power environment like a mobile device or Raspberry Pi, as the stick runs on power values as low as 1.2W.},
  eventtitle = {2019 {{IEEE Region}} 10 {{Symposium}} ({{TENSYMP}})},
  file = {C\:\\Users\\fried\\Zotero\\storage\\6ZRWVHNL\\Tiwari and Mondal - 2019 - NCS based ultra low power optimized machine learni.pdf},
  isbn = {978-1-72810-297-9},
  langid = {english}
}

@online{turnerDistillingPerformanceEnhanced2019,
  title = {Distilling with {{Performance Enhanced Students}}},
  author = {Turner, Jack and Crowley, Elliot J. and Radu, Valentin and Cano, José and Storkey, Amos and O'Boyle, Michael},
  date = {2019-03-07},
  url = {http://arxiv.org/abs/1810.10460},
  urldate = {2020-11-04},
  abstract = {The task of accelerating large neural networks on general purpose hardware has, in recent years, prompted the use of channel pruning to reduce network size. However, the efficacy of pruning based approaches has since been called into question. In this paper, we turn to distillation for model compression---specifically, attention transfer---and develop a simple method for discovering performance enhanced student networks. We combine channel saliency metrics with empirical observations of runtime performance to design more accurate networks for a given latency budget. We apply our methodology to residual and densely-connected networks, and show that we are able to find resource-efficient student networks on different hardware platforms while maintaining very high accuracy. These performance-enhanced student networks achieve up to 10\% boosts in top-1 ImageNet accuracy over their channel-pruned counterparts for the same inference time.},
  archivePrefix = {arXiv},
  eprint = {1810.10460},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\USMPE4XH\\Turner et al. - 2019 - Distilling with Performance Enhanced Students.pdf;C\:\\Users\\fried\\Zotero\\storage\\6IUA2X46\\1810.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Performance,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{umurogluFINNFrameworkFast2017,
  title = {{{FINN}}: {{A Framework}} for {{Fast}}, {{Scalable Binarized Neural Network Inference}}},
  shorttitle = {{{FINN}}},
  author = {Umuroglu, Yaman and Fraser, Nicholas J. and Gambardella, Giulio and Blott, Michaela and Leong, Philip and Jahre, Magnus and Vissers, Kees},
  date = {2017},
  journaltitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays - FPGA '17},
  pages = {65--74},
  doi = {10.1145/3020078.3021744},
  url = {http://arxiv.org/abs/1612.07119},
  urldate = {2020-10-01},
  abstract = {Research has shown that convolutional neural networks contain significant redundancy, and high classification accuracy can be obtained even when weights and activations are reduced from floating point to binary values. In this paper, we present Finn, a framework for building fast and flexible FPGA accelerators using a flexible heterogeneous streaming architecture. By utilizing a novel set of optimizations that enable efficient mapping of binarized neural networks to hardware, we implement fully connected, convolutional and pooling layers, with per-layer compute resources being tailored to user-provided throughput requirements. On a ZC706 embedded FPGA platform drawing less than 25 W total system power, we demonstrate up to 12.3 million image classifications per second with 0.31 µs latency on the MNIST dataset with 95.8\% accuracy, and 21906 image classifications per second with 283 µs latency on the CIFAR-10 and SVHN datasets with respectively 80.1\% and 94.9\% accuracy. To the best of our knowledge, ours are the fastest classification rates reported to date on these benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1612.07119},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\BIN9MDIZ\\Umuroglu et al. - 2017 - FINN A Framework for Fast, Scalable Binarized Neu.pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Hardware Architecture,Computer Science - Machine Learning},
  langid = {english}
}

@article{verbraekenSurveyDistributedMachine2020,
  ids = {verbraekenSurveyDistributedMachine2020a},
  title = {A {{Survey}} on {{Distributed Machine Learning}}},
  author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
  date = {2020-07},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  pages = {1--33},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3377454},
  url = {https://dl.acm.org/doi/10.1145/3377454},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\HC3VN74D\\Verbraeken et al. - 2020 - A Survey on Distributed Machine Learning.pdf;C\:\\Users\\fried\\Zotero\\storage\\ME4R7E6E\\Verbraeken et al. - 2020 - A Survey on Distributed Machine Learning.pdf},
  langid = {english},
  number = {2}
}

@article{vuducAutomaticPerformanceTuning,
  title = {Automatic {{Performance Tuning}} of {{Sparse Matrix Kernels}}},
  author = {Vuduc, Richard},
  pages = {455},
  file = {C\:\\Users\\fried\\Zotero\\storage\\RLLSEHDV\\Vuduc - Automatic Performance Tuning of Sparse Matrix Kern.pdf},
  langid = {english}
}

@online{wangBenchmarkingTPUGPU2019,
  title = {Benchmarking {{TPU}}, {{GPU}}, and {{CPU Platforms}} for {{Deep Learning}}},
  author = {Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David},
  date = {2019-10-22},
  url = {http://arxiv.org/abs/1907.10701},
  urldate = {2020-11-15},
  abstract = {Training deep learning models is compute-intensive and there is an industry-wide trend towards hardware specialization to improve performance. To systematically benchmark deep learning platforms, we introduce ParaDnn, a parameterized benchmark suite for deep learning that generates end-to-end models for fully connected (FC), convolutional (CNN), and recurrent (RNN) neural networks. Along with six real-world models, we benchmark Google's Cloud TPU v2/v3, NVIDIA's V100 GPU, and an Intel Skylake CPU platform. We take a deep dive into TPU architecture, reveal its bottlenecks, and highlight valuable lessons learned for future specialized system design. We also provide a thorough comparison of the platforms and find that each has unique strengths for some types of models. Finally, we quantify the rapid performance improvements that specialized software stacks provide for the TPU and GPU platforms.},
  archivePrefix = {arXiv},
  eprint = {1907.10701},
  eprinttype = {arxiv},
  file = {C\:\\Users\\fried\\Zotero\\storage\\EAR8RQA5\\Wang et al. - 2019 - Benchmarking TPU, GPU, and CPU Platforms for Deep .pdf;C\:\\Users\\fried\\Zotero\\storage\\8R9KUDXK\\1907.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Performance,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{wangDeepNeuralNetwork2019,
  ids = {wangDeepNeuralNetwork2019a},
  title = {Deep {{Neural Network Approximation}} for {{Custom Hardware}}: {{Where We}}'ve {{Been}}, {{Where We}}'re {{Going}}},
  shorttitle = {Deep {{Neural Network Approximation}} for {{Custom Hardware}}},
  author = {Wang, Erwei and Davis, James J. and Zhao, Ruizhe and Ng, Ho-Cheung and Niu, Xinyu and Luk, Wayne and Cheung, Peter Y. K. and Constantinides, George A.},
  date = {2019-05-31},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {52},
  pages = {1--39},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3309551},
  url = {https://dl.acm.org/doi/10.1145/3309551},
  urldate = {2020-10-01},
  file = {C\:\\Users\\fried\\Zotero\\storage\\H9SXTUMX\\Wang et al. - 2019 - Deep Neural Network Approximation for Custom Hardw.pdf;C\:\\Users\\fried\\Zotero\\storage\\X27S9LS8\\Wang et al. - 2019 - Deep Neural Network Approximation for Custom Hardw.pdf},
  langid = {english},
  number = {2}
}

@online{WelcomeBiologicallyInspired,
  title = {Welcome to {{Biologically Inspired Computation Coursework}}’s Documentation! — {{Biologically Inspired Computation Coursework}} 1.0 Documentation},
  url = {https://www2.macs.hw.ac.uk/~sf52/Bio-Comp-docs/html/index.html},
  urldate = {2020-11-22},
  file = {C\:\\Users\\fried\\Zotero\\storage\\3BGJJB2S\\index.html}
}

@article{williamsRooflineInsightfulVisual2009,
  title = {Roofline: An Insightful Visual Performance Model for Multicore Architectures},
  shorttitle = {Roofline},
  author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
  date = {2009-04-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {52},
  pages = {65--76},
  issn = {0001-0782},
  doi = {10.1145/1498765.1498785},
  url = {https://doi.org/10.1145/1498765.1498785},
  urldate = {2020-11-02},
  abstract = {The Roofline model offers insight on how to improve the performance of software and hardware.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\HGA6BECA\\Williams et al. - 2009 - Roofline an insightful visual performance model f.pdf;C\:\\Users\\fried\\Zotero\\storage\\MVV6XZ6L\\963540.pdf},
  number = {4}
}

@article{wuCompressingDeepNeural2020,
  ids = {wuCompressingDeepNeural2020a},
  title = {Compressing {{Deep Neural Networks With Sparse Matrix Factorization}}},
  author = {Wu, Kailun and Guo, Yiwen and Zhang, Changshui},
  date = {2020},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Netw. Learning Syst.},
  pages = {1--11},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2019.2946636},
  url = {https://ieeexplore.ieee.org/document/8901163/},
  urldate = {2020-10-02},
  abstract = {Modern deep neural networks (DNNs) are usually overparameterized and composed of a large number of learnable parameters. One of a few effective solutions attempts to compress DNN models via learning sparse weights and connections. In this article, we follow this line of research and present an alternative framework of learning sparse DNNs, with the assistance of matrix factorization. We provide an underlying principle for substituting the original parameter matrices with the multiplications of highly sparse ones, which constitutes the theoretical basis of our method. Experimental results demonstrate that our method substantially outperforms previous states of the arts for compressing various DNNs, giving rich empirical evidence in support of its effectiveness. It is also worth mentioning that, unlike many other works that focus on feedforward networks like multi-layer perceptrons and convolutional neural networks only, we also evaluate our method on a series of recurrent networks in practice.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\5L8MPPZT\\Wu et al. - 2020 - Compressing Deep Neural Networks With Sparse Matri.pdf;C\:\\Users\\fried\\Zotero\\storage\\6GL8KXZ2\\Wu et al. - 2020 - Compressing Deep Neural Networks With Sparse Matri.pdf},
  langid = {english}
}

@article{wulfHittingMemoryWall1995,
  title = {Hitting the Memory Wall: Implications of the Obvious},
  shorttitle = {Hitting the Memory Wall},
  author = {Wulf, Wm. A. and McKee, Sally A.},
  date = {1995-03-01},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  shortjournal = {SIGARCH Comput. Archit. News},
  volume = {23},
  pages = {20--24},
  issn = {0163-5964},
  doi = {10.1145/216585.216588},
  url = {https://doi.org/10.1145/216585.216588},
  urldate = {2020-11-02},
  file = {C\:\\Users\\fried\\Zotero\\storage\\KFJCD7JD\\Wulf and McKee - 1995 - Hitting the memory wall implications of the obvio.pdf},
  number = {1}
}

@article{yakovlevOracleAutoMLFast2020,
  ids = {yakovlevOracleAutoMLFast2020a},
  title = {Oracle {{AutoML}}: A Fast and Predictive {{AutoML}} Pipeline},
  shorttitle = {Oracle {{AutoML}}},
  author = {Yakovlev, Anatoly and Moghadam, Hesam Fathi and Moharrer, Ali and Cai, Jingxiao and Chavoshi, Nikan and Varadarajan, Venkatanathan and Agrawal, Sandeep R. and Idicula, Sam and Karnagel, Tomas and Jinturkar, Sanjay and Agarwal, Nipun},
  date = {2020-08},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {13},
  pages = {3166--3180},
  issn = {2150-8097},
  doi = {10.14778/3415478.3415542},
  url = {https://dl.acm.org/doi/10.14778/3415478.3415542},
  urldate = {2020-10-01},
  abstract = {Machine learning (ML) is at the forefront of the rising popularity of data-driven software applications. The resulting rapid proliferation of ML technology, explosive data growth, and shortage of data science expertise have caused the industry to face increasingly challenging demands to keep up with fast-paced develop-and-deploy model lifecycles. Recent academic and industrial research efforts have started to address this problem through automated machine learning (AutoML) pipelines and have focused on model performance as the first-order design objective. We present Oracle AutoML, a novel iteration-free AutoML pipeline designed to not only provide accurate models, but also in a shorter runtime. We are able to achieve these objectives by eliminating the need to continuously iterate over various pipeline configurations. In our feed-forward approach, each pipeline stage makes decisions based on metalearned proxy models that can predict candidate pipeline configuration performances before building the full final model. Our approach, which builds and tunes only the best candidate pipeline, achieves better scores at a fraction of the time compared to state-of-the-art open source AutoML tools, such as H2O and Auto-sklearn. This makes Oracle AutoML a prime candidate for addressing current industry challenges.},
  file = {C\:\\Users\\fried\\Zotero\\storage\\R8UW7U2G\\Yakovlev et al. - 2020 - Oracle AutoML a fast and predictive AutoML pipeli.pdf;C\:\\Users\\fried\\Zotero\\storage\\U4BUL9QI\\Yakovlev et al. - 2020 - Oracle AutoML a fast and predictive AutoML pipeli.pdf},
  langid = {english},
  number = {12}
}

@article{zotero-1,
  type = {article}
}

@book{zotero-167,
  type = {book}
}

@article{zotero-98,
  type = {article}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

